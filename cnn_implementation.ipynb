{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import rioxarray as rxr\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image \n",
    "from constants import labels\n",
    "\n",
    "from constants import labels\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import copy\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as nnf\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.nn.functional import relu\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenSendaiBenchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    An implementation of a PyTorch dataset for loading pairs of observable variables and ground truth labels.\n",
    "    Inspired by https://pytorch.org/tutorials/beginner/data_loading_tutorial.html.\n",
    "    \"\"\"\n",
    "    def __init__(self, obsvariables_path: str, groundtruth_path: str, country: str, signals: list, transform: transforms = None):\n",
    "        \"\"\"\n",
    "        Constructs an OpenSendaiBenchDataset.\n",
    "        :param obsvariables_path: Path to the source folder of observable variables\n",
    "        :param groundtruth_path: Path to the source folder of corresponding ground truth labels\n",
    "        :param transform: Callable transformation to apply to images upon loading\n",
    "        \"\"\"\n",
    "        self.obsvariables_path = obsvariables_path\n",
    "        self.groundtruth_path = groundtruth_path\n",
    "        self.country = country\n",
    "        self.signals = signals\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Implements the len(SeaIceDataset) magic method. Required to implement by Dataset superclass.\n",
    "        When training/testing, this method tells our training loop how much longer we have to go in our Dataset.\n",
    "        :return: Length of OpenSendaiBenchDataset\n",
    "        \"\"\"\n",
    "        return 100 #len(self.groundtruth_files)/labels[self.country]\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        \"\"\"\n",
    "        Implements the OpenSendaiBenchDataset[i] magic method. Required to implement by Dataset superclass.\n",
    "        When training/testing, this method is used to actually fetch data.\n",
    "        :param i: Index of which image pair to fetch\n",
    "        :return: Dictionary with pairs of observable variables and ground truth labels.\n",
    "        \"\"\"\n",
    "\n",
    "        obsvariable = np.zeros([len(self.signals),372,372])\n",
    "        obsvariable_8x8 = np.zeros([len(self.signals),8,8])\n",
    "        for s in range(len(self.signals)):\n",
    "            for file in glob.glob(str(os.getcwd()+self.obsvariables_path+\n",
    "                                    '**/'+self.country+'_*/'+self.country+'_'+\n",
    "                                    str(i)+'_'+'of_*/2019*_'+self.signals[s]+'.tif')):\n",
    "                a = cv2.imread(file, cv2.IMREAD_UNCHANGED)\n",
    "                a = a.reshape(1,a.shape[0],a.shape[1])\n",
    "                obsvariable[s,:,:] = a[0,0:372,0:372]\n",
    "            obsvariable_8x8[s,:,:] = cv2.resize(obsvariable[s,:,:], (8,8), interpolation = cv2.INTER_AREA)\n",
    "        groundtruth = np.zeros([len(labels[self.country]),372,372])\n",
    "        for w in range(len(labels[self.country])): # to make composite. in AFG, we got 5 bldgtypes\n",
    "            for file in glob.glob(str(os.getcwd()+self.groundtruth_path+\n",
    "                                      self.country+'*/tiles/images/'+\n",
    "                                      self.country+'_nbldg_'+labels[self.country][w]+'_'+str(i)+'_'+'of_'+'*.tif')):\n",
    "                a = cv2.imread(file, cv2.IMREAD_UNCHANGED)\n",
    "                a = cv2.resize(a, (372,372), interpolation = cv2.INTER_NEAREST)\n",
    "                a = a.reshape(1,a.shape[0],a.shape[1])\n",
    "                groundtruth[w,:,:] = a / ((372/8)**2) #scale factor\n",
    "\n",
    "        obsvariable = torch.from_numpy(obsvariable).float()\n",
    "        obsvariable_8x8 = torch.from_numpy(obsvariable_8x8).float()\n",
    "        groundtruth = torch.from_numpy(groundtruth).float()\n",
    "    \n",
    "        sample = {\"obsvariable\": obsvariable, \"groundtruth\": groundtruth}\n",
    "        if self.transform:\n",
    "            sample = {\"obsvariable\": self.transform(obsvariable),\n",
    "                      \"groundtruth\": self.transform(groundtruth).squeeze(0).long()}\n",
    "        return sample\n",
    "\n",
    "    def visualise(self, i):\n",
    "        \"\"\"\n",
    "        Allows us to visualise a particular SAR/chart pair.\n",
    "        :param i: Index of which image pair to visualise\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        sample = self[i]\n",
    "        fig1, axs1 = plt.subplots(1,len(self.signals))\n",
    "        for s in range(len(self.signals)):\n",
    "            axs1[s].imshow(sample['obsvariable'][s,:,:])\n",
    "            axs1[s].set_title(str(self.signals[s]))\n",
    "            axs1[s].set_xticks([])\n",
    "            axs1[s].set_yticks([])\n",
    "        plt.tight_layout()\n",
    " \n",
    "        fig2, axs2 = plt.subplots(1,len(labels[self.country]))\n",
    "        for w in range(len(labels[self.country])): \n",
    "            axs2[w].imshow(sample['groundtruth'][w,:,:])\n",
    "            axs2[w].set_title(labels[self.country][w])\n",
    "            axs2[w].set_xticks([])\n",
    "            axs2[w].set_yticks([])\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([372, 372])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = OpenSendaiBenchDataset( obsvariables_path=\"/obsvariables/\", \n",
    "                                        groundtruth_path=\"/groundtruth/\", \n",
    "                                        country='AFG', \n",
    "                                        signals = ['VH','VV','aerosol','blue','green','red','red1','red2','red3','nir','red4','vapor','swir1','swir2'])\n",
    "train_dataset[1]['groundtruth'][1,:,:].shape\n",
    "# dataset['groundtruth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 372, 372])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e11 = nn.Conv2d(14, 64, kernel_size=5, padding=1)\n",
    "xe11 = relu(e11(train_dataset[1]['obsvariable'])) # torch.Size([64, 370, 370])\n",
    "\n",
    "e12 = nn.Conv2d(64, 128, kernel_size=5, padding=1)\n",
    "xe12 = relu(e12(xe11)) # torch.Size([128, 368, 368])\n",
    "\n",
    "upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=6)\n",
    "xu1 = upconv1(xe12) # torch.Size([64, 370, 370])\n",
    "\n",
    "outconv = nn.Conv2d(64, 5, kernel_size=2)\n",
    "out = relu(outconv(xu1))\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.e11 = nn.Conv2d(14, 64, kernel_size=5, padding=1)\n",
    "        self.e12 = nn.Conv2d(64, 128, kernel_size=5, padding=1)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=6)\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xu1 = self.upconv1(xe12)\n",
    "        out = self.outconv(xu1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class UNet(nn.Module):\n",
    "#     def __init__(self, n_class:int):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.e11 = nn.Conv2d(14, 64, kernel_size=5, padding=1)\n",
    "#         self.e12 = nn.Conv2d(64, 128, kernel_size=5, padding=1)\n",
    "#         self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "#         self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor):\n",
    "\n",
    "#         xe11 = self.e11(x)\n",
    "#         xe12 = self.e12(xe11)\n",
    "#         xu1 = self.upconv1(xe12)\n",
    "#         out = self.outconv(xu1)\n",
    "\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class UNet(nn.Module):\n",
    "#     def __init__(self, n_class:int):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.e11 = nn.Conv2d(in_channels=14, out_channels=64, kernel_size=4, padding=1)\n",
    "#         self.e12 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, padding=1) \n",
    "#         self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.e21 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, padding=1)\n",
    "#         self.e22 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=4, padding=1) \n",
    "#         self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.e31 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, padding=1)\n",
    "#         self.e32 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=4, padding=1) \n",
    "#         self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "#         self.e41 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, padding=1)\n",
    "#         self.e42 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=4, padding=1) \n",
    "#         self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "#         self.e51 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=4, padding=1) \n",
    "#         self.e52 = nn.Conv2d(in_channels=1024, out_channels=n_class, kernel_size=12, padding=1) \n",
    "\n",
    "#     def forward(self, x: torch.Tensor):\n",
    "\n",
    "#         xe11 = relu(self.e11(x))\n",
    "#         xe12 = relu(self.e12(xe11))\n",
    "#         xp1 = self.pool1(xe12)\n",
    "\n",
    "#         xe21 = relu(self.e21(xp1))\n",
    "#         xe22 = relu(self.e22(xe21))\n",
    "#         xp2 = self.pool2(xe22)\n",
    "\n",
    "#         xe31 = relu(self.e31(xp2))\n",
    "#         xe32 = relu(self.e32(xe31))\n",
    "#         xp3 = self.pool3(xe32)\n",
    "\n",
    "#         xe41 = relu(self.e41(xp3))\n",
    "#         xe42 = relu(self.e42(xe41))\n",
    "#         xp4 = self.pool4(xe42)\n",
    "\n",
    "#         xe51 = relu(self.e51(xp4))\n",
    "#         xe52 = relu(self.e52(xe51))\n",
    "\n",
    "#         return xe52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = UNet(n_class=len(labels['AFG']))\n",
    "# output = model(train_dataset[1]['obsvariable'])\n",
    "# output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005],\n",
       "        [0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005],\n",
       "        [0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]['groundtruth'][1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.zeros([len(labels['AFG']),8,8])\n",
    "# for i in range(output.shape[0]):\n",
    "#     a[i,:,:] = cv2.resize(output[i,:,:].detach().numpy(), \n",
    "#                (8,8), interpolation = cv2.INTER_AREA) * ((372/8)**2) \n",
    "# a.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmentation(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    A LightningModule designed to perform image segmentation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 train_dataloader: DataLoader,\n",
    "                 val_dataloader: DataLoader,\n",
    "                 model: nn.Module,\n",
    "                 criterion: callable,\n",
    "                 learning_rate: float,\n",
    "                 metric: callable,\n",
    "                 country: str\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Construct a Segmentation LightningModule.\n",
    "        Note that we keep hyperparameters separate from dataloaders to prevent data leakage at test time.\n",
    "        :param train_dataloader: Dataloader with training data, left as None at test time\n",
    "        :param val_dataloader: Dataloader with validation data, left as None at test time\n",
    "        :param model: PyTorch model\n",
    "        :param criterion: PyTorch loss function against which to train model\n",
    "        :param learning_rate: Float learning rate for our optimiser\n",
    "        :param metric: PyTorch function for model evaluation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.learning_rate = learning_rate\n",
    "        self.metric = metric\n",
    "        self.country = country\n",
    "\n",
    "    def training_step(self, batch: dict, batch_idx: int):\n",
    "        \"\"\"\n",
    "        Perform a pass through a batch of training data.\n",
    "        :param batch: Batch of image pairs\n",
    "        :param batch_idx: Index of batch\n",
    "        :return: Loss from this batch of data for use in backprop\n",
    "        \"\"\"\n",
    "        x, y  = batch[\"obsvariable\"], batch[\"groundtruth\"]\n",
    "        y_hat = self.model(x)\n",
    "        # a_hat = nnf.interpolate(y_hat, size=(8, 8), \n",
    "        #                         mode='area') * 2162.25\n",
    "        # a = nnf.interpolate(y, size=(8, 8), \n",
    "        #                     mode='area') * 2162.25\n",
    "        # loss = torch.sqrt(self.criterion(a_hat,a)+1e-6)\n",
    "        loss = torch.sqrt(self.criterion(y_hat,y)+1e-6)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y  = batch[\"obsvariable\"], batch[\"groundtruth\"]\n",
    "        y_hat = self.model(x)\n",
    "        # a_hat = nnf.interpolate(y_hat, size=(8, 8), \n",
    "        #                         mode='area') * 2162.25\n",
    "        # a = nnf.interpolate(y, size=(8, 8), \n",
    "        #                     mode='area') * 2162.25\n",
    "        # loss = torch.sqrt(self.criterion(a_hat,a)+1e-6)\n",
    "        # metric = torch.sqrt(self.criterion(a_hat,a)+1e-6)\n",
    "        loss = torch.sqrt(self.criterion(y_hat,y)+1e-6)\n",
    "        metric = torch.sqrt(self.criterion(y_hat,y)+1e-6)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_metric\", metric)\n",
    "        return loss\n",
    "\n",
    "    def testing_step(self, batch, batch_idx):\n",
    "        x, y  = batch[\"obsvariable\"], batch[\"groundtruth\"]\n",
    "        y_hat = self.model(x)\n",
    "        # a_hat = nnf.interpolate(y_hat, size=(8, 8), \n",
    "        #                         mode='area') * 2162.25\n",
    "        # a = nnf.interpolate(y, size=(8, 8), \n",
    "        #                     mode='area') * 2162.25\n",
    "        # loss = torch.sqrt(self.criterion(a_hat,a)+1e-6)\n",
    "        # metric = torch.sqrt(self.criterion(a_hat,a)+1e-6)\n",
    "        loss = torch.sqrt(self.criterion(y_hat,y)+1e-6)\n",
    "        metric = torch.sqrt(self.criterion(y_hat,y)+1e-6)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_metric\", metric)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return {\n",
    "            \"optimizer\": optimizer\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = OpenSendaiBenchDataset( obsvariables_path=\"/obsvariables/\", \n",
    "                                        groundtruth_path=\"/groundtruth/\", \n",
    "                                        country='AFG', \n",
    "                                        signals = ['VH','VV','aerosol','blue','green','red','red1','red2','red3','nir','red4','vapor','swir1','swir2'])\n",
    "train_dataloader = DataLoader(train_dataset)\n",
    "val_dataset = OpenSendaiBenchDataset(   obsvariables_path=\"/obsvariables/\", \n",
    "                                        groundtruth_path=\"/groundtruth/\", \n",
    "                                        country='AFG', \n",
    "                                        signals = ['VH','VV','aerosol','blue','green','red','red1','red2','red3','nir','red4','vapor','swir1','swir2'])\n",
    "val_dataloader = DataLoader(val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = UNet(n_class=len(labels['AFG']))\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 1e-4\n",
    "metric = nn.MSELoss()\n",
    "segmenter = Segmentation(train_dataloader, val_dataloader, \n",
    "                         model, criterion, learning_rate, metric,\n",
    "                         country='AFG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:9sml4f95) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_metric</td><td>█▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_loss</td><td>0.00495</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_loss</td><td>0.00392</td></tr><tr><td>val_metric</td><td>0.00392</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peach-wood-54</strong> at: <a href='https://wandb.ai/opensendaibench/opensendaibench/runs/9sml4f95' target=\"_blank\">https://wandb.ai/opensendaibench/opensendaibench/runs/9sml4f95</a><br/>Synced 7 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240117_203332-9sml4f95/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:9sml4f95). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/wandb/run-20240117_204147-vupikrhd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/opensendaibench/opensendaibench/runs/vupikrhd' target=\"_blank\">colorful-galaxy-55</a></strong> to <a href='https://wandb.ai/opensendaibench/opensendaibench' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/opensendaibench/opensendaibench' target=\"_blank\">https://wandb.ai/opensendaibench/opensendaibench</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/opensendaibench/opensendaibench/runs/vupikrhd' target=\"_blank\">https://wandb.ai/opensendaibench/opensendaibench/runs/vupikrhd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"opensendaibench\")  # initialise wandb\n",
    "wandb_logger = pl.loggers.WandbLogger(project=\"opensendaibench\")  # create a logger object\n",
    "wandb_logger.watch(model, log=\"all\", log_freq=10)  # tell our logger to watch the model we are training to track parameters and gradients\n",
    "wandb_logger.experiment.config.update(  # log experimental config items of interest\n",
    "    {\n",
    "        \"learning_rate\": learning_rate\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | model     | UNet    | 523 K \n",
      "1 | criterion | MSELoss | 0     \n",
      "2 | metric    | MSELoss | 0     \n",
      "--------------------------------------\n",
      "523 K     Trainable params\n",
      "0         Non-trainable params\n",
      "523 K     Total params\n",
      "2.095     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 100/100 [00:11<00:00,  8.90it/s, v_num=krhd]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 100/100 [00:11<00:00,  8.88it/s, v_num=krhd]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "trainer = pl.Trainer(max_epochs=25) \n",
    "trainer.logger = wandb_logger  \n",
    "trainer.callbacks.append(ModelCheckpoint(monitor=\"val_loss\")) \n",
    "trainer.fit(segmenter, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[220], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m output \u001b[38;5;241m=\u001b[39m model(val_dataset[j][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobsvariable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mmap\u001b[39m \u001b[38;5;241m=\u001b[39m output[i,:,:]\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(val_dataset[j][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroundtruth\u001b[39m\u001b[38;5;124m\"\u001b[39m][i,:,:])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 5 is out of bounds for axis 0 with size 5"
     ]
    }
   ],
   "source": [
    "i = 5\n",
    "j = 5\n",
    "output = model(val_dataset[j]['obsvariable'])\n",
    "output = output.detach().numpy()\n",
    "map = output[i,:,:]\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(val_dataset[j][\"groundtruth\"][i,:,:])\n",
    "plt.title(\"True Chart\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "img = plt.imshow(map)\n",
    "plt.colorbar(img)\n",
    "plt.title(\"Predicted Chart\")\n",
    "\n",
    "print(val_dataset[j][\"groundtruth\"][i,:,:])\n",
    "print(map.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opensendaibench",
   "language": "python",
   "name": "opensendaibench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
