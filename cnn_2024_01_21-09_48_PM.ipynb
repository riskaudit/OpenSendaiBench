{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from constants import labels\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import rioxarray as rxr\n",
    "import copy\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from PIL import Image \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "import torch.nn.functional as nnf\n",
    "from torch.nn.functional import relu\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from constants import labels\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import rioxarray as rxr\n",
    "import copy\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from PIL import Image \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "import torch.nn.functional as nnf\n",
    "from torch.nn.functional import relu\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "device = torch.device(\"mps\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class OpenSendaiBenchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    An implementation of a PyTorch dataset for loading pairs of observable variables and ground truth labels.\n",
    "    Inspired by https://pytorch.org/tutorials/beginner/data_loading_tutorial.html.\n",
    "    \"\"\"\n",
    "    def __init__(self, obsvariables_path: str, groundtruth_path: str, country: str, signals: list, transform: transforms = None):\n",
    "        \"\"\"\n",
    "        Constructs an OpenSendaiBenchDataset.\n",
    "        :param obsvariables_path: Path to the source folder of observable variables\n",
    "        :param groundtruth_path: Path to the source folder of corresponding ground truth labels\n",
    "        :param transform: Callable transformation to apply to images upon loading\n",
    "        \"\"\"\n",
    "        self.obsvariables_path = obsvariables_path\n",
    "        self.groundtruth_path = groundtruth_path\n",
    "        self.country = country\n",
    "        self.signals = signals\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Implements the len(SeaIceDataset) magic method. Required to implement by Dataset superclass.\n",
    "        When training/testing, this method tells our training loop how much longer we have to go in our Dataset.\n",
    "        :return: Length of OpenSendaiBenchDataset\n",
    "        \"\"\"\n",
    "        return 100 #len(self.groundtruth_files)/labels[self.country]\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        \"\"\"\n",
    "        Implements the OpenSendaiBenchDataset[i] magic method. Required to implement by Dataset superclass.\n",
    "        When training/testing, this method is used to actually fetch data.\n",
    "        :param i: Index of which image pair to fetch\n",
    "        :return: Dictionary with pairs of observable variables and ground truth labels.\n",
    "        \"\"\"\n",
    "\n",
    "        obsvariable = np.zeros([len(self.signals),368,368])\n",
    "        for s in range(len(self.signals)):\n",
    "            for file in glob.glob(str(os.getcwd()+self.obsvariables_path+\n",
    "                                    '**/'+self.country+'_*/'+self.country+'_'+\n",
    "                                    str(i)+'_'+'of_*/2019*_'+self.signals[s]+'.tif')):\n",
    "                a = cv2.imread(file, cv2.IMREAD_UNCHANGED)\n",
    "                a = cv2.resize(a, (368,368), interpolation = cv2.INTER_NEAREST)\n",
    "                obsvariable[s,:,:] = a.reshape(1,a.shape[0],a.shape[1])\n",
    "                \n",
    "        groundtruth = np.zeros([len(labels[self.country]),8,8])\n",
    "        for w in range(len(labels[self.country])): \n",
    "            for file in glob.glob(str(os.getcwd()+self.groundtruth_path+\n",
    "                                      self.country+'*/tiles/images/'+\n",
    "                                      self.country+'_nbldg_'+labels[self.country][w]+'_'+str(i)+'_'+'of_'+'*.tif')):\n",
    "                a = cv2.imread(file, cv2.IMREAD_UNCHANGED)\n",
    "                groundtruth[w,:,:] = a.reshape(1,a.shape[0],a.shape[1])\n",
    "\n",
    "        obsvariable = torch.from_numpy(obsvariable).float() #.unsqueeze(0)\n",
    "        # obsvariable_8x8 = torch.from_numpy(obsvariable_8x8).float()\n",
    "        groundtruth = torch.from_numpy(groundtruth).float() #.unsqueeze(0)\n",
    "    \n",
    "        sample = {\"obsvariable\": obsvariable, \"groundtruth\": groundtruth}\n",
    "        if self.transform:\n",
    "            sample = {\"obsvariable\": self.transform(obsvariable),\n",
    "                      \"groundtruth\": self.transform(groundtruth).squeeze(0).long()}\n",
    "        return sample\n",
    "\n",
    "    def visualise(self, i):\n",
    "        \"\"\"\n",
    "        Allows us to visualise a particular SAR/chart pair.\n",
    "        :param i: Index of which image pair to visualise\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        sample = self[i]\n",
    "        fig1, axs1 = plt.subplots(1,len(self.signals))\n",
    "        for s in range(len(self.signals)):\n",
    "            axs1[s].imshow(sample['obsvariable'][s,:,:])\n",
    "            axs1[s].set_title(str(self.signals[s]))\n",
    "            axs1[s].set_xticks([])\n",
    "            axs1[s].set_yticks([])\n",
    "        plt.tight_layout()\n",
    " \n",
    "        fig2, axs2 = plt.subplots(1,len(labels[self.country]))\n",
    "        for w in range(len(labels[self.country])): \n",
    "            axs2[w].imshow(sample['groundtruth'][w,:,:])\n",
    "            axs2[w].set_title(labels[self.country][w])\n",
    "            axs2[w].set_xticks([])\n",
    "            axs2[w].set_yticks([])\n",
    "        plt.tight_layout()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "train_ds = OpenSendaiBenchDataset( obsvariables_path=\"/obsvariables/\", \n",
    "                                        groundtruth_path=\"/groundtruth/\", \n",
    "                                        country='AFG', \n",
    "                                        signals = ['blue','green','red'])\n",
    "                                        # signals = ['VH','VV','aerosol','blue','green','red','red1','red2','red3','nir','red4','vapor','swir1','swir2'])\n",
    "print(train_ds[1]['groundtruth'].shape)\n",
    "print(train_ds[1]['obsvariable'].shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 8, 8])\n",
      "torch.Size([3, 368, 368])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=10, shuffle=True)\n",
    "len(train_dl)\n",
    "train_dl.dataset[1]['groundtruth'].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 8])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1)\n",
    "        self.e12 = nn.Conv2d(64, 128, kernel_size=5, padding=1)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=6)\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xu1 = self.upconv1(xe12)\n",
    "        out = F.avg_pool2d(self.outconv(xu1),(46,46),46) #, divisor_override=1)\n",
    "\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "model = UNet(n_class=len(labels['AFG'])).to(device)\n",
    "print(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "UNet(\n",
      "  (e11): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (e12): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (upconv1): ConvTranspose2d(128, 64, kernel_size=(6, 6), stride=(1, 1))\n",
      "  (outconv): Conv2d(64, 1, kernel_size=(2, 2), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "summary(model, input_size=(3, 368, 368))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Conv2d: 1-1                            4,864\n",
      "├─Conv2d: 1-2                            204,928\n",
      "├─ConvTranspose2d: 1-3                   294,976\n",
      "├─Conv2d: 1-4                            257\n",
      "=================================================================\n",
      "Total params: 505,025\n",
      "Trainable params: 505,025\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Conv2d: 1-1                            4,864\n",
       "├─Conv2d: 1-2                            204,928\n",
       "├─ConvTranspose2d: 1-3                   294,976\n",
       "├─Conv2d: 1-4                            257\n",
       "=================================================================\n",
       "Total params: 505,025\n",
       "Trainable params: 505,025\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "train_ds[1]['groundtruth'].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 8])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "train_dl.dataset[1]['obsvariable'].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 368, 368])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# class UNet(nn.Module):\n",
    "#     def __init__(self, n_class):\n",
    "#         super().__init__()\n",
    "#         self.e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1)\n",
    "#         self.e12 = nn.Conv2d(64, 128, kernel_size=5, padding=1)\n",
    "#         self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=6)\n",
    "#         self.outconv = nn.Conv2d(64, n_class, kernel_size=2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         xe11 = relu(self.e11(x))\n",
    "#         xe12 = relu(self.e12(xe11))\n",
    "#         xu1 = self.upconv1(xe12)\n",
    "#         out = F.avg_pool2d(self.outconv(xu1),(46,46),46) #, divisor_override=1)\n",
    "\n",
    "#         return out\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=3, padding=1) \n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=1) \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    " \n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=1) \n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=1) \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    " \n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=1) \n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=1) \n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=1) \n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    " \n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1) \n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1) \n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = relu(self.e21(xp1))\n",
    "        xe22 = relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = relu(self.e31(xp2))\n",
    "        xe32 = relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = relu(self.e41(xp3))\n",
    "        xe42 = relu(self.e42(xe41))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = relu(self.e51(xp4))\n",
    "        xe52 = relu(self.e52(xe51))\n",
    "        \n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52)\n",
    "        xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "        xd11 = relu(self.d11(xu11))\n",
    "        xd12 = relu(self.d12(xd11))\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "        xd21 = relu(self.d21(xu22))\n",
    "        xd22 = relu(self.d22(xd21))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xu33 = torch.cat([xu3, xe22], dim=1)\n",
    "        xd31 = relu(self.d31(xu33))\n",
    "        xd32 = relu(self.d32(xd31))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xu44 = torch.cat([xu4, xe12], dim=1)\n",
    "        xd41 = relu(self.d41(xu44))\n",
    "        xd42 = relu(self.d42(xd41))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd42)\n",
    "\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "model = UNet(n_class=len(labels['AFG'])).to(device)\n",
    "print(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "UNet(\n",
      "  (e11): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (e12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (e21): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (e22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (e31): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (e32): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (e41): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (e42): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (e51): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (e52): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (upconv1): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (d11): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (d12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (upconv2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (d21): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (d22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (d31): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (d32): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (upconv4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (d41): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (d42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (outconv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "summary(model, input_size=(3, 368, 368))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Conv2d: 1-1                            1,792\n",
      "├─Conv2d: 1-2                            36,928\n",
      "├─MaxPool2d: 1-3                         --\n",
      "├─Conv2d: 1-4                            73,856\n",
      "├─Conv2d: 1-5                            147,584\n",
      "├─MaxPool2d: 1-6                         --\n",
      "├─Conv2d: 1-7                            295,168\n",
      "├─Conv2d: 1-8                            590,080\n",
      "├─MaxPool2d: 1-9                         --\n",
      "├─Conv2d: 1-10                           1,180,160\n",
      "├─Conv2d: 1-11                           2,359,808\n",
      "├─MaxPool2d: 1-12                        --\n",
      "├─Conv2d: 1-13                           4,719,616\n",
      "├─Conv2d: 1-14                           9,438,208\n",
      "├─ConvTranspose2d: 1-15                  2,097,664\n",
      "├─Conv2d: 1-16                           4,719,104\n",
      "├─Conv2d: 1-17                           2,359,808\n",
      "├─ConvTranspose2d: 1-18                  524,544\n",
      "├─Conv2d: 1-19                           1,179,904\n",
      "├─Conv2d: 1-20                           590,080\n",
      "├─ConvTranspose2d: 1-21                  131,200\n",
      "├─Conv2d: 1-22                           295,040\n",
      "├─Conv2d: 1-23                           147,584\n",
      "├─ConvTranspose2d: 1-24                  32,832\n",
      "├─Conv2d: 1-25                           73,792\n",
      "├─Conv2d: 1-26                           36,928\n",
      "├─Conv2d: 1-27                           65\n",
      "=================================================================\n",
      "Total params: 31,031,745\n",
      "Trainable params: 31,031,745\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Conv2d: 1-1                            1,792\n",
       "├─Conv2d: 1-2                            36,928\n",
       "├─MaxPool2d: 1-3                         --\n",
       "├─Conv2d: 1-4                            73,856\n",
       "├─Conv2d: 1-5                            147,584\n",
       "├─MaxPool2d: 1-6                         --\n",
       "├─Conv2d: 1-7                            295,168\n",
       "├─Conv2d: 1-8                            590,080\n",
       "├─MaxPool2d: 1-9                         --\n",
       "├─Conv2d: 1-10                           1,180,160\n",
       "├─Conv2d: 1-11                           2,359,808\n",
       "├─MaxPool2d: 1-12                        --\n",
       "├─Conv2d: 1-13                           4,719,616\n",
       "├─Conv2d: 1-14                           9,438,208\n",
       "├─ConvTranspose2d: 1-15                  2,097,664\n",
       "├─Conv2d: 1-16                           4,719,104\n",
       "├─Conv2d: 1-17                           2,359,808\n",
       "├─ConvTranspose2d: 1-18                  524,544\n",
       "├─Conv2d: 1-19                           1,179,904\n",
       "├─Conv2d: 1-20                           590,080\n",
       "├─ConvTranspose2d: 1-21                  131,200\n",
       "├─Conv2d: 1-22                           295,040\n",
       "├─Conv2d: 1-23                           147,584\n",
       "├─ConvTranspose2d: 1-24                  32,832\n",
       "├─Conv2d: 1-25                           73,792\n",
       "├─Conv2d: 1-26                           36,928\n",
       "├─Conv2d: 1-27                           65\n",
       "=================================================================\n",
       "Total params: 31,031,745\n",
       "Trainable params: 31,031,745\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "loss_func = nn.L1Loss()\n",
    "iterator = iter(train_dl)\n",
    "\n",
    "for batch_idx in range(len(train_dl)):\n",
    "    data_batch = next(iterator)\n",
    "    xb = data_batch['obsvariable'].type(torch.float).to(device)\n",
    "    yb = data_batch['groundtruth'].type(torch.float).to(device)\n",
    "    out = model(xb)\n",
    "    loss = loss_func(out, yb)\n",
    "    print(loss)\n",
    "    print(loss.item())\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([10, 1, 8, 8])) that is different to the input size (torch.Size([10, 1, 368, 368])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (368) must match the size of tensor b (8) at non-singleton dimension 3",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py\u001b[0m in \u001b[0;36mline 10\n\u001b[1;32m      <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=248'>249</a>\u001b[0m yb \u001b[39m=\u001b[39m data_batch[\u001b[39m'\u001b[39m\u001b[39mgroundtruth\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mfloat)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=249'>250</a>\u001b[0m out \u001b[39m=\u001b[39m model(xb)\n\u001b[0;32m---> <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=250'>251</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_func(out, yb)\n\u001b[1;32m     <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=251'>252</a>\u001b[0m \u001b[39mprint\u001b[39m(loss)\n\u001b[1;32m     <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=252'>253</a>\u001b[0m \u001b[39mprint\u001b[39m(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1515'>1516</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1516'>1517</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1517'>1518</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1521'>1522</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1522'>1523</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1523'>1524</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1524'>1525</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1525'>1526</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1526'>1527</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1528'>1529</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1529'>1530</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/loss.py:101\u001b[0m, in \u001b[0;36mL1Loss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/loss.py?line=99'>100</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/loss.py?line=100'>101</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39ml1_loss(\u001b[39minput\u001b[39m, target, reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreduction)\n",
      "File \u001b[0;32m~/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/functional.py:3297\u001b[0m, in \u001b[0;36ml1_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/functional.py?line=3293'>3294</a>\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/functional.py?line=3294'>3295</a>\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/functional.py?line=3296'>3297</a>\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbroadcast_tensors(\u001b[39minput\u001b[39m, target)\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/functional.py?line=3297'>3298</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39ml1_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/functional.py:73\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/functional.py?line=70'>71</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/functional.py?line=71'>72</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[0;32m---> <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/functional.py?line=72'>73</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mbroadcast_tensors(tensors)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (368) must match the size of tensor b (8) at non-singleton dimension 3"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1)\n",
    "        self.e12 = nn.Conv2d(64, 128, kernel_size=5, padding=1)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=6)\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xu1 = self.upconv1(xe12)\n",
    "        out = F.avg_pool2d(self.outconv(xu1),(46,46),46) #, divisor_override=1)\n",
    "\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "model = UNet(n_class=len(labels['AFG'])).to(device)\n",
    "print(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "UNet(\n",
      "  (e11): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (e12): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (upconv1): ConvTranspose2d(128, 64, kernel_size=(6, 6), stride=(1, 1))\n",
      "  (outconv): Conv2d(64, 1, kernel_size=(2, 2), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "summary(model, input_size=(3, 368, 368))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Conv2d: 1-1                            4,864\n",
      "├─Conv2d: 1-2                            204,928\n",
      "├─ConvTranspose2d: 1-3                   294,976\n",
      "├─Conv2d: 1-4                            257\n",
      "=================================================================\n",
      "Total params: 505,025\n",
      "Trainable params: 505,025\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Conv2d: 1-1                            4,864\n",
       "├─Conv2d: 1-2                            204,928\n",
       "├─ConvTranspose2d: 1-3                   294,976\n",
       "├─Conv2d: 1-4                            257\n",
       "=================================================================\n",
       "Total params: 505,025\n",
       "Trainable params: 505,025\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "loss_func = nn.L1Loss()\n",
    "iterator = iter(train_dl)\n",
    "\n",
    "for batch_idx in range(len(train_dl)):\n",
    "    data_batch = next(iterator)\n",
    "    xb = data_batch['obsvariable'].type(torch.float).to(device)\n",
    "    yb = data_batch['groundtruth'].type(torch.float).to(device)\n",
    "    out = model(xb)\n",
    "    loss = loss_func(out, yb)\n",
    "    print(loss)\n",
    "    print(loss.item())\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(78.9881, device='mps:0', grad_fn=<MeanBackward0>)\n",
      "78.9880599975586\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "loss_func = nn.L1Loss()\n",
    "iterator = iter(train_dl)\n",
    "\n",
    "for batch_idx in range(len(train_dl)):\n",
    "    data_batch = next(iterator)\n",
    "    xb = data_batch['obsvariable'].type(torch.float).to(device)\n",
    "    yb = data_batch['groundtruth'].type(torch.float).to(device)\n",
    "    out = model(xb)\n",
    "    loss = loss_func(out, yb)\n",
    "    print(loss)\n",
    "    print(loss.item())\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(72.1795, device='mps:0', grad_fn=<MeanBackward0>)\n",
      "72.1794662475586\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "loss_func = nn.L1Loss()\n",
    "iterator = iter(train_dl)\n",
    "\n",
    "for batch_idx in range(len(train_dl)):\n",
    "    data_batch = next(iterator)\n",
    "    xb = data_batch['obsvariable'].type(torch.float).to(device)\n",
    "    print(xb.shape)\n",
    "    yb = data_batch['groundtruth'].type(torch.float).to(device)\n",
    "    out = model(xb)\n",
    "    loss = loss_func(out, yb)\n",
    "    print(loss)\n",
    "    print(loss.item())\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 3, 368, 368])\n",
      "tensor(73.9866, device='mps:0', grad_fn=<MeanBackward0>)\n",
      "73.98661041259766\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "xb"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[1766.2483, 1440.2413, 1399.0068,  ..., 1303.8055, 1303.8055,\n",
       "           1369.1389],\n",
       "          [1690.9794, 1421.5310, 1404.9172,  ..., 1365.9028, 1365.9028,\n",
       "           1416.0555],\n",
       "          [1517.5862, 1438.6897, 1438.6897,  ..., 1560.4166, 1560.4166,\n",
       "           1469.4722],\n",
       "          ...,\n",
       "          [1923.4492, 1963.4160, 2001.5840,  ..., 1465.1111, 1420.0000,\n",
       "           1526.6389],\n",
       "          [1926.1086, 1980.9124, 2018.4745,  ..., 1301.7639, 1270.3611,\n",
       "           1245.3472],\n",
       "          [2051.1680, 2105.7590, 2113.1094,  ..., 1279.8472, 1288.6666,\n",
       "           1198.3889]],\n",
       "\n",
       "         [[2259.1792, 1868.9724, 1832.2759,  ..., 1748.8055, 1748.8055,\n",
       "           1820.7639],\n",
       "          [2159.4690, 1834.2345, 1838.3448,  ..., 1820.9028, 1820.9028,\n",
       "           1871.7639],\n",
       "          [1929.0690, 1839.9724, 1839.9724,  ..., 2046.1111, 2046.1111,\n",
       "           1939.6666],\n",
       "          ...,\n",
       "          [2425.5652, 2463.6060, 2513.2263,  ..., 1966.6945, 1906.1250,\n",
       "           2021.3055],\n",
       "          [2433.5796, 2490.4817, 2596.7590,  ..., 1731.6945, 1711.0834,\n",
       "           1685.2639],\n",
       "          [2582.4526, 2647.2847, 2708.6570,  ..., 1717.8334, 1729.0972,\n",
       "           1615.5972]],\n",
       "\n",
       "         [[2750.1104, 2192.5310, 2068.2896,  ..., 1995.3889, 1995.3889,\n",
       "           2089.3889],\n",
       "          [2622.2207, 2148.0688, 2071.0000,  ..., 2104.0972, 2104.0972,\n",
       "           2179.1667],\n",
       "          [2312.8413, 2171.8965, 2171.8965,  ..., 2411.0278, 2411.0278,\n",
       "           2298.7500],\n",
       "          ...,\n",
       "          [2891.0435, 2950.2483, 3016.7590,  ..., 2403.6667, 2309.1943,\n",
       "           2461.9167],\n",
       "          [2924.6377, 3000.8467, 3129.4453,  ..., 2114.6250, 2077.5557,\n",
       "           2047.5555],\n",
       "          [3105.0657, 3185.4744, 3267.5767,  ..., 2058.7500, 2070.0139,\n",
       "           1964.7361]]],\n",
       "\n",
       "\n",
       "        [[[1518.2324, 1518.2324, 1534.4296,  ..., 2207.6128, 2239.5986,\n",
       "           2182.5281],\n",
       "          [1553.1056, 1553.1056, 1594.9296,  ..., 2205.1619, 2234.6338,\n",
       "           2203.6973],\n",
       "          [1614.7113, 1637.4084, 1637.4084,  ..., 2196.7888, 2228.9578,\n",
       "           2206.9648],\n",
       "          ...,\n",
       "          [1941.4366, 1912.0352, 1872.7253,  ..., 1969.4718, 1648.2113,\n",
       "           1620.5282],\n",
       "          [1761.5422, 1757.5000, 1749.8522,  ..., 1977.7676, 1643.2394,\n",
       "           1580.4789],\n",
       "          [1680.7676, 1661.5282, 1664.6127,  ..., 1882.1831, 1656.3029,\n",
       "           1571.6971]],\n",
       "\n",
       "         [[1668.4929, 1668.4929, 1689.0986,  ..., 2686.0142, 2730.6902,\n",
       "           2633.5916],\n",
       "          [1690.1620, 1690.1620, 1753.3380,  ..., 2676.7043, 2715.9578,\n",
       "           2666.2676],\n",
       "          [1773.7535, 1813.7394, 1813.7394,  ..., 2664.6338, 2703.5774,\n",
       "           2676.0422],\n",
       "          ...,\n",
       "          [2393.5635, 2345.7043, 2297.1128,  ..., 2423.7888, 2042.3662,\n",
       "           1999.3873],\n",
       "          [2169.7605, 2152.9858, 2145.0281,  ..., 2440.9297, 2033.1267,\n",
       "           1958.6267],\n",
       "          [2055.7466, 2028.0422, 2037.3662,  ..., 2303.3804, 2039.1971,\n",
       "           1947.8591]],\n",
       "\n",
       "         [[1821.5000, 1821.5000, 1845.2183,  ..., 3144.1902, 3180.4858,\n",
       "           3058.7395],\n",
       "          [1840.8733, 1840.8733, 1921.6267,  ..., 3125.5845, 3164.8943,\n",
       "           3090.8450],\n",
       "          [1935.7535, 1985.6267, 1985.6267,  ..., 3106.9297, 3145.8240,\n",
       "           3103.3311],\n",
       "          ...,\n",
       "          [2689.2957, 2622.5281, 2551.0986,  ..., 2758.0845, 2248.4436,\n",
       "           2200.5142],\n",
       "          [2385.3521, 2372.8733, 2344.0845,  ..., 2812.0212, 2238.1338,\n",
       "           2136.8804],\n",
       "          [2231.8381, 2203.6267, 2207.2817,  ..., 2650.1550, 2252.5000,\n",
       "           2113.6057]]],\n",
       "\n",
       "\n",
       "        [[[2381.0413, 2356.3794, 2276.7986,  ..., 2121.1311, 2129.9036,\n",
       "           2145.4827],\n",
       "          [2271.5725, 2273.0068, 2232.7173,  ..., 2115.6138, 2132.4482,\n",
       "           2152.0620],\n",
       "          [2188.8137, 2194.0000, 2171.5725,  ..., 2110.6965, 2134.0208,\n",
       "           2166.6277],\n",
       "          ...,\n",
       "          [1571.4482, 1459.3655, 1438.9586,  ..., 1673.5385, 1680.4615,\n",
       "           1658.4846],\n",
       "          [1501.6276, 1383.3241, 1380.8483,  ..., 1673.0538, 1680.0385,\n",
       "           1687.6769],\n",
       "          [1482.8690, 1371.7725, 1365.0620,  ..., 1685.5461, 1690.6000,\n",
       "           1669.7000]],\n",
       "\n",
       "         [[2931.1655, 2909.2551, 2780.1448,  ..., 2562.7656, 2575.0413,\n",
       "           2590.6001],\n",
       "          [2783.4070, 2793.3242, 2742.7380,  ..., 2553.4482, 2578.7656,\n",
       "           2605.0483],\n",
       "          [2683.9036, 2677.4346, 2662.7173,  ..., 2547.8345, 2580.8828,\n",
       "           2622.3311],\n",
       "          ...,\n",
       "          [1923.8414, 1792.9794, 1762.7587,  ..., 1924.1615, 1937.9923,\n",
       "           1917.0000],\n",
       "          [1836.8069, 1703.0896, 1695.9104,  ..., 1924.4154, 1934.0231,\n",
       "           1953.2539],\n",
       "          [1811.9104, 1686.8345, 1674.6138,  ..., 1950.1769, 1943.9462,\n",
       "           1921.0769]],\n",
       "\n",
       "         [[3574.5930, 3544.9587, 3404.6206,  ..., 3096.3518, 3122.2966,\n",
       "           3142.9587],\n",
       "          [3412.0483, 3426.7173, 3364.2415,  ..., 3081.3655, 3117.4622,\n",
       "           3151.0068],\n",
       "          [3298.5930, 3291.0000, 3268.6758,  ..., 3063.2966, 3110.4827,\n",
       "           3163.4207],\n",
       "          ...,\n",
       "          [2083.5449, 1891.6759, 1835.4690,  ..., 2186.5769, 2181.1538,\n",
       "           2146.0769],\n",
       "          [1957.9242, 1760.7931, 1733.6276,  ..., 2192.7153, 2195.4460,\n",
       "           2200.2847],\n",
       "          [1927.5862, 1733.4276, 1707.2827,  ..., 2195.7615, 2211.1538,\n",
       "           2157.0154]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[1209.0834, 1251.6805, 1259.0416,  ..., 1332.5634, 1340.6478,\n",
       "           1339.2778],\n",
       "          [1206.6111, 1236.6945, 1293.8611,  ..., 1187.0845, 1204.4225,\n",
       "           1192.2222],\n",
       "          [1232.1127, 1271.6111, 1336.5416,  ..., 1197.8334, 1204.7778,\n",
       "           1142.5416],\n",
       "          ...,\n",
       "          [1696.2361, 1696.2361, 1691.8611,  ..., 1698.5972, 1689.1805,\n",
       "           1689.1805],\n",
       "          [1697.2291, 1697.2291, 1686.9445,  ..., 1742.2014, 1684.3611,\n",
       "           1684.3611],\n",
       "          [1697.6945, 1697.6945, 1699.9722,  ..., 1735.1528, 1634.7222,\n",
       "           1634.7222]],\n",
       "\n",
       "         [[1626.1111, 1652.3472, 1659.9861,  ..., 1610.9028, 1624.0416,\n",
       "           1676.4861],\n",
       "          [1634.6805, 1653.9305, 1691.8611,  ..., 1449.3055, 1506.8309,\n",
       "           1550.1831],\n",
       "          [1609.6528, 1673.9861, 1734.6805,  ..., 1512.9166, 1570.0564,\n",
       "           1499.3944],\n",
       "          ...,\n",
       "          [2149.1875, 2149.1875, 2132.2778,  ..., 2228.2986, 2214.6318,\n",
       "           2214.6318],\n",
       "          [2148.0972, 2148.0972, 2137.2986,  ..., 2277.3125, 2195.5000,\n",
       "           2195.5000],\n",
       "          [2157.7917, 2157.7917, 2155.9375,  ..., 2249.3611, 2101.1597,\n",
       "           2101.1597]],\n",
       "\n",
       "         [[1707.9722, 1804.6805, 1826.9584,  ..., 1765.3472, 1785.2222,\n",
       "           1829.0555],\n",
       "          [1697.8889, 1743.5555, 1845.0555,  ..., 1544.0000, 1582.2916,\n",
       "           1618.0416],\n",
       "          [1691.2361, 1772.4166, 1892.1389,  ..., 1577.4028, 1615.4166,\n",
       "           1531.5555],\n",
       "          ...,\n",
       "          [2575.0625, 2575.0625, 2559.7292,  ..., 2730.1389, 2684.0417,\n",
       "           2684.0417],\n",
       "          [2572.9443, 2572.9443, 2569.2917,  ..., 2779.7222, 2631.0278,\n",
       "           2631.0278],\n",
       "          [2589.2014, 2589.2014, 2586.5833,  ..., 2729.3333, 2491.4307,\n",
       "           2491.4307]]],\n",
       "\n",
       "\n",
       "        [[[1738.2329, 1792.6438, 1792.6438,  ..., 1549.8494, 1531.0959,\n",
       "           1531.0959],\n",
       "          [1758.0548, 1796.7946, 1796.7946,  ..., 1531.6576, 1521.1644,\n",
       "           1521.1644],\n",
       "          [1859.2192, 1900.0548, 1900.0548,  ..., 1530.1506, 1531.6849,\n",
       "           1531.6849],\n",
       "          ...,\n",
       "          [1617.7534, 1633.5685, 1582.8287,  ..., 1495.7192, 1498.3904,\n",
       "           1497.0822],\n",
       "          [1628.6713, 1639.2329, 1584.2534,  ..., 1497.1506, 1491.1506,\n",
       "           1493.6713],\n",
       "          [1648.6096, 1639.7192, 1599.9520,  ..., 1505.1644, 1496.4932,\n",
       "           1506.5273]],\n",
       "\n",
       "         [[2022.8494, 2099.6301, 2099.6301,  ..., 1808.8494, 1789.3014,\n",
       "           1789.3014],\n",
       "          [2045.6301, 2093.3972, 2093.3972,  ..., 1788.8356, 1782.6301,\n",
       "           1782.6301],\n",
       "          [2178.7671, 2217.5479, 2217.5479,  ..., 1785.3287, 1781.1233,\n",
       "           1781.1233],\n",
       "          ...,\n",
       "          [1900.3219, 1959.9586, 1933.2275,  ..., 1761.4727, 1765.1301,\n",
       "           1758.6096],\n",
       "          [1916.8835, 1937.0137, 1896.9041,  ..., 1763.8287, 1762.0343,\n",
       "           1757.1986],\n",
       "          [1945.8219, 1934.8767, 1911.1713,  ..., 1771.7671, 1761.3904,\n",
       "           1770.0411]],\n",
       "\n",
       "         [[2271.3152, 2356.3562, 2356.3562,  ..., 1873.2466, 1841.5068,\n",
       "           1841.5068],\n",
       "          [2287.3972, 2329.9727, 2329.9727,  ..., 1849.5068, 1831.6438,\n",
       "           1831.6438],\n",
       "          [2414.4658, 2457.5205, 2457.5205,  ..., 1837.8356, 1832.4520,\n",
       "           1832.4520],\n",
       "          ...,\n",
       "          [2008.0411, 2044.0205, 1953.0685,  ..., 1856.4589, 1864.7793,\n",
       "           1860.7603],\n",
       "          [2029.6028, 2054.7329, 1952.3082,  ..., 1861.7397, 1853.4110,\n",
       "           1863.8151],\n",
       "          [2075.5615, 2062.1233, 2012.8562,  ..., 1871.4384, 1860.1506,\n",
       "           1884.3699]]],\n",
       "\n",
       "\n",
       "        [[[1639.7465, 1735.6056, 1872.4929,  ..., 1967.5775, 1982.1831,\n",
       "           1998.7324],\n",
       "          [1605.7606, 1705.1831, 1836.4648,  ..., 1929.1831, 1942.0564,\n",
       "           1963.9014],\n",
       "          [1598.2113, 1694.2676, 1832.8309,  ..., 1944.7324, 1949.9155,\n",
       "           1956.1831],\n",
       "          ...,\n",
       "          [2497.9014, 2497.9014, 2478.6479,  ..., 2074.0703, 1748.5775,\n",
       "           1748.5775],\n",
       "          [2552.3098, 2552.3098, 2588.0281,  ..., 2054.2957, 1737.5211,\n",
       "           1660.1127],\n",
       "          [2557.3098, 2557.3098, 2563.6057,  ..., 2095.8027, 1763.3380,\n",
       "           1667.1409]],\n",
       "\n",
       "         [[1968.6198, 2070.7605, 2230.4648,  ..., 2349.0000, 2382.1267,\n",
       "           2395.7888],\n",
       "          [1932.6902, 2037.9718, 2185.0000,  ..., 2308.3662, 2331.6902,\n",
       "           2347.0845],\n",
       "          [1918.4366, 2031.3098, 2187.8027,  ..., 2326.7324, 2336.3098,\n",
       "           2340.6338],\n",
       "          ...,\n",
       "          [2962.9858, 2962.9858, 2943.7747,  ..., 2483.6902, 2097.6619,\n",
       "           2097.6619],\n",
       "          [3024.2534, 3024.2534, 3070.1973,  ..., 2455.7183, 2077.9719,\n",
       "           1976.5352],\n",
       "          [3024.3662, 3024.3662, 3037.7747,  ..., 2491.1550, 2092.9719,\n",
       "           1984.3098]],\n",
       "\n",
       "         [[1997.1409, 2164.5635, 2375.8450,  ..., 2629.8733, 2660.8169,\n",
       "           2686.3098],\n",
       "          [1917.5775, 2090.1689, 2314.1267,  ..., 2574.0422, 2599.4929,\n",
       "           2624.1409],\n",
       "          [1901.0564, 2079.0422, 2308.5493,  ..., 2600.8169, 2611.3381,\n",
       "           2613.6338],\n",
       "          ...,\n",
       "          [3368.0000, 3368.0000, 3351.2957,  ..., 2741.7324, 2180.8311,\n",
       "           2180.8311],\n",
       "          [3446.6760, 3446.6760, 3509.2676,  ..., 2698.0142, 2136.0845,\n",
       "           2004.0000],\n",
       "          [3452.4507, 3452.4507, 3470.4648,  ..., 2733.2676, 2154.1973,\n",
       "           2013.2394]]]], device='mps:0')"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "xb.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 368, 368])"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "e11 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "xe11 = relu(e11(xb))\n",
    "xe11.shape"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Input type (MPSFloatType) and weight type (torch.FloatTensor) should be the same",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py\u001b[0m in \u001b[0;36mline 3\n\u001b[1;32m      <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=147'>148</a>\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[1;32m      <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=148'>149</a>\u001b[0m e11 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mConv2d(\u001b[39m3\u001b[39m, \u001b[39m64\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=149'>150</a>\u001b[0m xe11 \u001b[39m=\u001b[39m relu(e11(xb))\n\u001b[1;32m      <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=150'>151</a>\u001b[0m xe11\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1515'>1516</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1516'>1517</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1517'>1518</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1521'>1522</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1522'>1523</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1523'>1524</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1524'>1525</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1525'>1526</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1526'>1527</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1528'>1529</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1529'>1530</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=458'>459</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=459'>460</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conv_forward(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=451'>452</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=452'>453</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=453'>454</a>\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=454'>455</a>\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=455'>456</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\u001b[39minput\u001b[39m, weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=456'>457</a>\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (MPSFloatType) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "e11 = nn.Conv2d(3, 64, kernel_size=3, padding=1).to(device)\n",
    "xe11 = relu(e11(xb))\n",
    "xe11.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([10, 64, 368, 368])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "e11 = nn.Conv2d(3, 64, kernel_size=3, padding=1).to(device) # torch.Size([10, 64, 368, 368])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=3, padding=1).to(device)\n",
    "xe12 = relu(e12(xe11))\n",
    "xe12.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([10, 64, 368, 368])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 368, 368])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device)\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device)\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=3, padding=1).to(device) # \n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 182, 182])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 182, 182])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=3, padding=1).to(device)\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 180, 180])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device)\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device)\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1).to(device)\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 89, 89])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1).to(device) # torch.Size([10, 256, 89, 89])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=3, padding=1).to(device)\n",
    "xe32 = relu(e32(e32))\n",
    "print(xe32.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 89, 89])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (Conv2d, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!Conv2d!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!Conv2d!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py\u001b[0m in \u001b[0;36mline 31\n\u001b[1;32m     <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=174'>175</a>\u001b[0m \u001b[39mprint\u001b[39m(xe31\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=176'>177</a>\u001b[0m e32 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mConv2d(\u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=177'>178</a>\u001b[0m xe32 \u001b[39m=\u001b[39m relu(e32(e32))\n\u001b[1;32m     <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=178'>179</a>\u001b[0m \u001b[39mprint\u001b[39m(xe32\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1515'>1516</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1516'>1517</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1517'>1518</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1521'>1522</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1522'>1523</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1523'>1524</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1524'>1525</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1525'>1526</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1526'>1527</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1528'>1529</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1529'>1530</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=458'>459</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=459'>460</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conv_forward(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=451'>452</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=452'>453</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=453'>454</a>\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=454'>455</a>\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=455'>456</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\u001b[39minput\u001b[39m, weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=456'>457</a>\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n",
      "\u001b[0;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (Conv2d, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!Conv2d!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!Conv2d!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1).to(device) # torch.Size([10, 256, 89, 89])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=3, padding=1).to(device)\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 89, 89])\n",
      "torch.Size([10, 256, 89, 89])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 89, 89])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device)\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device)\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=3, padding=1).to(device)\n",
    "xe41 = relu(e41(xp3))\n",
    "print(xe41.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 42, 42])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=3, padding=1).to(device) # torch.Size([10, 512, 42, 42])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=3, padding=1).to(device)\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 42, 42])\n",
      "torch.Size([10, 512, 42, 42])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 42, 42])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device)\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device)\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1).to(device)\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 19, 19])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device)\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) \n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=5, stride=2).to(device)\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 33, 33])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=5, stride=1).to(device)\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 19, 19])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=5, stride=2).to(device)\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 33, 33])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=6, stride=2).to(device)\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 34, 34])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2).to(device)\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 32, 32])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=5, stride=2).to(device)\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 33, 33])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=6, stride=2).to(device)\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 34, 34])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=5, stride=3, padding=1).to(device)\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 45, 45])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=5, padding=1).to(device)\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 17, 17])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=5, padding=2).to(device)\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 15, 15])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=5, padding=1).to(device)\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 17, 17])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=5, stride=1, padding=1).to(device)\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 17, 17])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=5, stride=2, padding=1).to(device)\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 31, 31])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=5, stride=3).to(device)\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 47, 47])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=3).to(device)\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 46, 46])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=3, stride=3).to(device)\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 45, 45])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=8, stride=3).to(device)\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 50, 50])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=8, stride=2).to(device)\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 36, 36])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=8, stride=3).to(device)\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 50, 50])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=8, stride=2).to(device)\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 36, 36])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device)\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "print(xu11.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1).to(device)\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 38, 38])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1)\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Input type (MPSFloatType) and weight type (torch.FloatTensor) should be the same",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py\u001b[0m in \u001b[0;36mline 74\n\u001b[1;32m     <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=217'>218</a>\u001b[0m \u001b[39mprint\u001b[39m(xd11\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=219'>220</a>\u001b[0m d12 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mConv2d(\u001b[39m512\u001b[39m, \u001b[39m512\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=220'>221</a>\u001b[0m xd12 \u001b[39m=\u001b[39m relu(d12(xd11))\n\u001b[1;32m     <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=221'>222</a>\u001b[0m \u001b[39mprint\u001b[39m(xd12\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1515'>1516</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1516'>1517</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1517'>1518</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1521'>1522</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1522'>1523</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1523'>1524</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1524'>1525</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1525'>1526</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1526'>1527</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1528'>1529</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1529'>1530</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=458'>459</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=459'>460</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conv_forward(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=451'>452</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=452'>453</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=453'>454</a>\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=454'>455</a>\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=455'>456</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\u001b[39minput\u001b[39m, weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=456'>457</a>\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (MPSFloatType) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device)\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2).to(device) \n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 68, 68])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=10, stride=2).to(device) \n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 76, 76])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=10, stride=3).to(device) \n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 109, 109])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=9, stride=3).to(device) \n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 108, 108])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=3).to(device) \n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 102, 102])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=5, stride=2).to(device) \n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 71, 71])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=10, stride=2).to(device) \n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 76, 76])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=15, stride=2).to(device) \n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 81, 81])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=15, stride=2, padding=1).to(device) \n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 79, 79])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=15, stride=2, padding=2).to(device) \n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 77, 77])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=15, stride=2, padding=0).to(device) \n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 81, 81])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=15, stride=2).to(device) \n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 81, 81])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) \n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 85, 85])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) \n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "print(xu22.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 512, 85, 85])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "# d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "# xd12 = relu(d12(xd11))\n",
    "# print(xd12.shape)\n",
    "\n",
    "# upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "# xu2 = upconv2(xd12)\n",
    "# print(xu2.shape)\n",
    "\n",
    "# xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "# print(xu22.shape)\n",
    "\n",
    "# d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 38, 38])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=4, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "# d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "# xd12 = relu(d12(xd11))\n",
    "# print(xd12.shape)\n",
    "\n",
    "# upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "# xu2 = upconv2(xd12)\n",
    "# print(xu2.shape)\n",
    "\n",
    "# xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "# print(xu22.shape)\n",
    "\n",
    "# d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 37, 37])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=4, padding=1).to(device) # torch.Size([10, 512, 37, 37])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=4, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "# upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "# xu2 = upconv2(xd12)\n",
    "# print(xu2.shape)\n",
    "\n",
    "# xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "# print(xu22.shape)\n",
    "\n",
    "# d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 37, 37])\n",
      "torch.Size([10, 512, 36, 36])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1).to(device) # torch.Size([10, 512, 37, 37])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "# upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "# xu2 = upconv2(xd12)\n",
    "# print(xu2.shape)\n",
    "\n",
    "# xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "# print(xu22.shape)\n",
    "\n",
    "# d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 38, 38])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 37, 37])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "# upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "# xu2 = upconv2(xd12)\n",
    "# print(xu2.shape)\n",
    "\n",
    "# xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "# print(xu22.shape)\n",
    "\n",
    "# d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)\n",
    "\n",
    "# xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "# print(xu22.shape)\n",
    "\n",
    "# d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 85, 85])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "print(xu22.shape)\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device)\n",
    "xd21 = relu(d21(xu22))\n",
    "print(xd21.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 512, 85, 85])\n",
      "torch.Size([10, 256, 83, 83])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "print(xu22.shape)\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "print(xd21.shape)\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device)\n",
    "xd22 = relu(d22(xd21))\n",
    "print(xd22.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 512, 85, 85])\n",
      "torch.Size([10, 256, 83, 83])\n",
      "torch.Size([10, 256, 81, 81])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "print(xu22.shape)\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "print(xd21.shape)\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "print(xd22.shape)\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2).to(device)\n",
    "xu3 = upconv3(xd22)\n",
    "print(xu3.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 512, 85, 85])\n",
      "torch.Size([10, 256, 83, 83])\n",
      "torch.Size([10, 256, 81, 81])\n",
      "torch.Size([10, 128, 162, 162])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "print(xu22.shape)\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "print(xd21.shape)\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "print(xd22.shape)\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=1).to(device)\n",
    "xu3 = upconv3(xd22)\n",
    "print(xu3.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 512, 85, 85])\n",
      "torch.Size([10, 256, 83, 83])\n",
      "torch.Size([10, 256, 81, 81])\n",
      "torch.Size([10, 128, 82, 82])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "print(xu22.shape)\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "print(xd21.shape)\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "print(xd22.shape)\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=).to(device)\n",
    "xu3 = upconv3(xd22)\n",
    "print(xu3.shape)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-95-11feccd42bd7>, line 92)",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[95], line 92\u001b[0;36m\u001b[0m\n\u001b[0;31m    upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=).to(device)\u001b[0m\n\u001b[0m                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "print(xu22.shape)\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "print(xd21.shape)\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "print(xd22.shape)\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2).to(device)\n",
    "xu3 = upconv3(xd22)\n",
    "print(xu3.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 512, 85, 85])\n",
      "torch.Size([10, 256, 83, 83])\n",
      "torch.Size([10, 256, 81, 81])\n",
      "torch.Size([10, 128, 162, 162])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "print(xu22.shape)\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "print(xd21.shape)\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "print(xd22.shape)\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device)\n",
    "xu3 = upconv3(xd22)\n",
    "print(xu3.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 512, 85, 85])\n",
      "torch.Size([10, 256, 83, 83])\n",
      "torch.Size([10, 256, 81, 81])\n",
      "torch.Size([10, 128, 178, 178])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "print(xu22.shape)\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "print(xd21.shape)\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "print(xd22.shape)\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device)\n",
    "xu3 = upconv3(xd22)\n",
    "print(xu3.shape)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1)\n",
    "print(xu33.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 512, 85, 85])\n",
      "torch.Size([10, 256, 83, 83])\n",
      "torch.Size([10, 256, 81, 81])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 256, 178, 178])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "print(xu22.shape)\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "print(xd21.shape)\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "print(xd22.shape)\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "print(xu3.shape)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "print(xu33.shape)\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device)\n",
    "xd31 = relu(d31(xu33))\n",
    "print(xd31.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 512, 85, 85])\n",
      "torch.Size([10, 256, 83, 83])\n",
      "torch.Size([10, 256, 81, 81])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 256, 178, 178])\n",
      "torch.Size([10, 128, 176, 176])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "print(xu22.shape)\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "print(xd21.shape)\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "print(xd22.shape)\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "print(xu3.shape)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "print(xu33.shape)\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "print(xd31.shape)\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1)\n",
    "xd32 = relu(d32(xu31))\n",
    "print(xd32.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 512, 85, 85])\n",
      "torch.Size([10, 256, 83, 83])\n",
      "torch.Size([10, 256, 81, 81])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 256, 178, 178])\n",
      "torch.Size([10, 128, 176, 176])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'xu31' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py\u001b[0m in \u001b[0;36mline 104\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=247'>248</a>\u001b[0m \u001b[39mprint\u001b[39m(xd31\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=249'>250</a>\u001b[0m d32 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mConv2d(\u001b[39m128\u001b[39m, \u001b[39m128\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=250'>251</a>\u001b[0m xd32 \u001b[39m=\u001b[39m relu(d32(xu31))\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=251'>252</a>\u001b[0m \u001b[39mprint\u001b[39m(xd32\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xu31' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "print(xu22.shape)\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "print(xd21.shape)\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "print(xd22.shape)\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "print(xu3.shape)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "print(xu33.shape)\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "print(xd31.shape)\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1)\n",
    "xd32 = relu(d32(xd31))\n",
    "print(xd32.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 512, 85, 85])\n",
      "torch.Size([10, 256, 83, 83])\n",
      "torch.Size([10, 256, 81, 81])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 256, 178, 178])\n",
      "torch.Size([10, 128, 176, 176])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Input type (MPSFloatType) and weight type (torch.FloatTensor) should be the same",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py\u001b[0m in \u001b[0;36mline 104\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=247'>248</a>\u001b[0m \u001b[39mprint\u001b[39m(xd31\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=249'>250</a>\u001b[0m d32 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mConv2d(\u001b[39m128\u001b[39m, \u001b[39m128\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=250'>251</a>\u001b[0m xd32 \u001b[39m=\u001b[39m relu(d32(xd31))\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/Desktop/PhD/GitHub/OpenSendaiBench/cnn.py?line=251'>252</a>\u001b[0m \u001b[39mprint\u001b[39m(xd32\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1515'>1516</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1516'>1517</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1517'>1518</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1521'>1522</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1522'>1523</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1523'>1524</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1524'>1525</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1525'>1526</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1526'>1527</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1528'>1529</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/module.py?line=1529'>1530</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=458'>459</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=459'>460</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conv_forward(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=451'>452</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=452'>453</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=453'>454</a>\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=454'>455</a>\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=455'>456</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\u001b[39minput\u001b[39m, weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    <a href='file:///Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/nn/modules/conv.py?line=456'>457</a>\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (MPSFloatType) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "print(xu22.shape)\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "print(xd21.shape)\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "print(xd22.shape)\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "print(xu3.shape)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "print(xu33.shape)\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "print(xd31.shape)\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device)\n",
    "xd32 = relu(d32(xd31))\n",
    "print(xd32.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 512, 85, 85])\n",
      "torch.Size([10, 256, 83, 83])\n",
      "torch.Size([10, 256, 81, 81])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 256, 178, 178])\n",
      "torch.Size([10, 128, 176, 176])\n",
      "torch.Size([10, 128, 174, 174])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "print(xu22.shape)\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "print(xd21.shape)\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "print(xd22.shape)\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "print(xu3.shape)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "print(xu33.shape)\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "print(xd31.shape)\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "print(xd32.shape)\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2).to(device) \n",
    "xu4 = upconv4(xd32)\n",
    "print(xu4.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 512, 85, 85])\n",
      "torch.Size([10, 256, 83, 83])\n",
      "torch.Size([10, 256, 81, 81])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 256, 178, 178])\n",
      "torch.Size([10, 128, 176, 176])\n",
      "torch.Size([10, 128, 174, 174])\n",
      "torch.Size([10, 64, 348, 348])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "364-348"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "print(xu22.shape)\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "print(xd21.shape)\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "print(xd22.shape)\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "print(xu3.shape)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "print(xu33.shape)\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "print(xd31.shape)\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "print(xd32.shape)\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device) \n",
    "xu4 = upconv4(xd32)\n",
    "print(xu4.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 512, 85, 85])\n",
      "torch.Size([10, 256, 83, 83])\n",
      "torch.Size([10, 256, 81, 81])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 256, 178, 178])\n",
      "torch.Size([10, 128, 176, 176])\n",
      "torch.Size([10, 128, 174, 174])\n",
      "torch.Size([10, 64, 364, 364])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "print(xu22.shape)\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "print(xd21.shape)\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "print(xd22.shape)\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "print(xu3.shape)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "print(xu33.shape)\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "print(xd31.shape)\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "print(xd32.shape)\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "print(xu4.shape)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1)\n",
    "print(xu44.shape)\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 512, 85, 85])\n",
      "torch.Size([10, 256, 83, 83])\n",
      "torch.Size([10, 256, 81, 81])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 256, 178, 178])\n",
      "torch.Size([10, 128, 176, 176])\n",
      "torch.Size([10, 128, 174, 174])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 128, 364, 364])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "print(xu22.shape)\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "print(xd21.shape)\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "print(xd22.shape)\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "print(xu3.shape)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "print(xu33.shape)\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "print(xd31.shape)\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "print(xd32.shape)\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "print(xu4.shape)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1)\n",
    "print(xu44.shape)\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=5, padding=1).to(device)\n",
    "xd41 = relu(d41(xu44))\n",
    "print(xd41.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 512, 85, 85])\n",
      "torch.Size([10, 256, 83, 83])\n",
      "torch.Size([10, 256, 81, 81])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 256, 178, 178])\n",
      "torch.Size([10, 128, 176, 176])\n",
      "torch.Size([10, 128, 174, 174])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 128, 364, 364])\n",
      "torch.Size([10, 64, 362, 362])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "print(xu22.shape)\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "print(xd21.shape)\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "print(xd22.shape)\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "print(xu3.shape)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "print(xu33.shape)\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "print(xd31.shape)\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "print(xd32.shape)\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "print(xu4.shape)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "print(xu44.shape)\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "print(xd41.shape)\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device)\n",
    "xd42 = relu(d42(xd41))\n",
    "print(xd42.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 512, 85, 85])\n",
      "torch.Size([10, 256, 83, 83])\n",
      "torch.Size([10, 256, 81, 81])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 256, 178, 178])\n",
      "torch.Size([10, 128, 176, 176])\n",
      "torch.Size([10, 128, 174, 174])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 128, 364, 364])\n",
      "torch.Size([10, 64, 362, 362])\n",
      "torch.Size([10, 64, 360, 360])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "print(xe11.shape)\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "print(xe12.shape)\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "print(xp1.shape)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "print(xe21.shape)\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "print(xe22.shape)\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "print(xp2.shape)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "print(xe31.shape)\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "print(xe32.shape)\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "print(xp3.shape)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "print(xe41.shape)\n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "print(xe42.shape)\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "print(xp4.shape)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "print(xe51.shape)\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "print(xe52.shape)\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "print(xu1.shape)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "print(xu11.shape)\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "print(xd11.shape)\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "print(xd12.shape)\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "print(xu2.shape)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "print(xu22.shape)\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "print(xd21.shape)\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "print(xd22.shape)\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "print(xu3.shape)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "print(xu33.shape)\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "print(xd31.shape)\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "print(xd32.shape)\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "print(xu4.shape)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "print(xu44.shape)\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "print(xd41.shape)\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "print(xd42.shape)\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=1).to(device)\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 64, 366, 366])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 64, 182, 182])\n",
      "torch.Size([10, 128, 180, 180])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 128, 89, 89])\n",
      "torch.Size([10, 256, 87, 87])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 256, 42, 42])\n",
      "torch.Size([10, 512, 40, 40])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 512, 19, 19])\n",
      "torch.Size([10, 1024, 17, 17])\n",
      "torch.Size([10, 1024, 15, 15])\n",
      "torch.Size([10, 512, 38, 38])\n",
      "torch.Size([10, 1024, 38, 38])\n",
      "torch.Size([10, 512, 36, 36])\n",
      "torch.Size([10, 512, 34, 34])\n",
      "torch.Size([10, 256, 85, 85])\n",
      "torch.Size([10, 512, 85, 85])\n",
      "torch.Size([10, 256, 83, 83])\n",
      "torch.Size([10, 256, 81, 81])\n",
      "torch.Size([10, 128, 178, 178])\n",
      "torch.Size([10, 256, 178, 178])\n",
      "torch.Size([10, 128, 176, 176])\n",
      "torch.Size([10, 128, 174, 174])\n",
      "torch.Size([10, 64, 364, 364])\n",
      "torch.Size([10, 128, 364, 364])\n",
      "torch.Size([10, 64, 362, 362])\n",
      "torch.Size([10, 64, 360, 360])\n",
      "torch.Size([10, 1, 360, 360])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "print(xd22.shape)\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=1).to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 256, 81, 81])\n",
      "torch.Size([10, 1, 360, 360])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=1).to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 360, 360])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "xb.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 368, 368])"
      ]
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "368/46"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=9).to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 352, 352])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=9, stride=2).to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 176, 176])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=9, stride=1).to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 352, 352])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=9, padding=1).to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 354, 354])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=9, padding=2).to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 356, 356])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=9, padding=25.to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (<ipython-input-119-5683a265b06f>, line 93)",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[119], line 93\u001b[0;36m\u001b[0m\n\u001b[0;31m    outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=9, padding=25.to(device) # torch.Size([10, 1, 360, 360])\u001b[0m\n\u001b[0m                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=9, padding=5).to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 362, 362])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=9, padding=6).to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 364, 364])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=12, padding=6).to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 361, 361])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=3, padding=6).to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 370, 370])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=3, padding=5).to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 368, 368])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=3, padding=2).to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 362, 362])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=8, padding=2).to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 357, 357])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=2, padding=2).to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 363, 363])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=2, padding=1).to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 361, 361])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=2, stride=1).to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 359, 359])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=2, padding=3).to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 365, 365])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=2, padding=4).to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 367, 367])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=3).to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 362, 362])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "model = UNet(n_class=len(labels['AFG'])).to(device)\n",
    "print(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "UNet(\n",
      "  (e11): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (e12): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (upconv1): ConvTranspose2d(128, 64, kernel_size=(6, 6), stride=(1, 1))\n",
      "  (outconv): Conv2d(64, 1, kernel_size=(2, 2), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=3).to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 362, 362])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1).to(device) # torch.Size([10, 64, 362, 362])\n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1).to(device) # torch.Size([10, 64, 360, 360])\n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=3).to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 362, 362])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1).to(device) \n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1).to(device) \n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=2).to(device) # torch.Size([10, 1, 360, 360])\n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 363, 363])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1).to(device) \n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1).to(device) \n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=2, padding=5).to(device) \n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 373, 373])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1).to(device) \n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1).to(device) \n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=6, padding=5).to(device) \n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 369, 369])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "source": [
    "## encoder\n",
    "\n",
    "e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "xe11 = relu(e11(xb))\n",
    "\n",
    "e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "xe12 = relu(e12(xe11))\n",
    "\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "xp1 = pool1(xe12)\n",
    "\n",
    "e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "xe21 = relu(e21(xp1))\n",
    "\n",
    "e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xe22 = relu(e22(xe21))\n",
    "\n",
    "pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "xp2 = pool2(xe22)\n",
    "\n",
    "e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "xe31 = relu(e31(xp2))\n",
    "\n",
    "e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xe32 = relu(e32(xe31))\n",
    "\n",
    "pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "xp3 = pool3(xe32)\n",
    "\n",
    "e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "xe41 = relu(e41(xp3)) \n",
    "\n",
    "e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xe42 = relu(e42(xe41))\n",
    "\n",
    "pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "xp4 = pool4(xe42)\n",
    "\n",
    "e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "xe51 = relu(e51(xp4))\n",
    "\n",
    "e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "xe52 = relu(e52(xe51))\n",
    "\n",
    "## decoder\n",
    "upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "xu1 = upconv1(xe52)\n",
    "\n",
    "xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "xd11 = relu(d11(xu11))\n",
    "\n",
    "d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "xd12 = relu(d12(xd11))\n",
    "\n",
    "upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "xu2 = upconv2(xd12)\n",
    "\n",
    "xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "xd21 = relu(d21(xu22))\n",
    "\n",
    "d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "xd22 = relu(d22(xd21))\n",
    "\n",
    "upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "xu3 = upconv3(xd22)\n",
    "\n",
    "xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "xd31 = relu(d31(xu33))\n",
    "\n",
    "d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "xd32 = relu(d32(xd31))\n",
    "\n",
    "upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "xu4 = upconv4(xd32)\n",
    "\n",
    "xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1).to(device) \n",
    "xd41 = relu(d41(xu44))\n",
    "\n",
    "d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1).to(device) \n",
    "xd42 = relu(d42(xd41))\n",
    "\n",
    "outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=7, padding=5).to(device) \n",
    "out = outconv(xd42)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 368, 368])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "source": [
    "# class UNet(nn.Module):\n",
    "#     def __init__(self, n_class):\n",
    "#         super().__init__()\n",
    "#         self.e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1)\n",
    "#         self.e12 = nn.Conv2d(64, 128, kernel_size=5, padding=1)\n",
    "#         self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=6)\n",
    "#         self.outconv = nn.Conv2d(64, n_class, kernel_size=2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         xe11 = relu(self.e11(x))\n",
    "#         xe12 = relu(self.e12(xe11))\n",
    "#         xu1 = self.upconv1(xe12)\n",
    "#         out = F.avg_pool2d(self.outconv(xu1),(46,46),46) #, divisor_override=1)\n",
    "\n",
    "#         return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "## encoder\n",
    "\n",
    "# e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 366, 366])\n",
    "# xe11 = relu(e11(xb))\n",
    "\n",
    "# e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1).to(device) # torch.Size([10, 64, 364, 364])\n",
    "# xe12 = relu(e12(xe11))\n",
    "\n",
    "# pool1 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 64, 182, 182])\n",
    "# xp1 = pool1(xe12)\n",
    "\n",
    "# e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 180, 180])\n",
    "# xe21 = relu(e21(xp1))\n",
    "\n",
    "# e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 178, 178])\n",
    "# xe22 = relu(e22(xe21))\n",
    "\n",
    "# pool2 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 128, 89, 89])\n",
    "# xp2 = pool2(xe22)\n",
    "\n",
    "# e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 87, 87])\n",
    "# xe31 = relu(e31(xp2))\n",
    "\n",
    "# e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 85, 85])\n",
    "# xe32 = relu(e32(xe31))\n",
    "\n",
    "# pool3 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 256, 42, 42])\n",
    "# xp3 = pool3(xe32)\n",
    "\n",
    "# e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 40, 40])\n",
    "# xe41 = relu(e41(xp3)) \n",
    "\n",
    "# e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 38, 38])\n",
    "# xe42 = relu(e42(xe41))\n",
    "\n",
    "# pool4 = nn.MaxPool2d(kernel_size=2, stride=2).to(device) # torch.Size([10, 512, 19, 19])\n",
    "# xp4 = pool4(xe42)\n",
    "\n",
    "# e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 17, 17])\n",
    "# xe51 = relu(e51(xp4))\n",
    "\n",
    "# e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1).to(device) # torch.Size([10, 1024, 15, 15])\n",
    "# xe52 = relu(e52(xe51))\n",
    "\n",
    "# ## decoder\n",
    "# upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2).to(device) # torch.Size([10, 512, 38, 38])\n",
    "# xu1 = upconv1(xe52)\n",
    "\n",
    "# xu11 = torch.cat([xu1, xe42], dim=1) # torch.Size([10, 1024, 38, 38])\n",
    "\n",
    "# d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 36, 36])\n",
    "# xd11 = relu(d11(xu11))\n",
    "\n",
    "# d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1).to(device) # torch.Size([10, 512, 34, 34])\n",
    "# xd12 = relu(d12(xd11))\n",
    "\n",
    "# upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2).to(device) # torch.Size([10, 256, 85, 85])\n",
    "# xu2 = upconv2(xd12)\n",
    "\n",
    "# xu22 = torch.cat([xu2, xe32], dim=1) # torch.Size([10, 512, 85, 85])\n",
    "\n",
    "# d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 83, 83])\n",
    "# xd21 = relu(d21(xu22))\n",
    "\n",
    "# d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1).to(device) # torch.Size([10, 256, 81, 81])\n",
    "# xd22 = relu(d22(xd21))\n",
    "\n",
    "# upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2).to(device) # torch.Size([10, 128, 178, 178])\n",
    "# xu3 = upconv3(xd22)\n",
    "\n",
    "# xu33 = torch.cat([xu3, xe22], dim=1) # torch.Size([10, 256, 178, 178])\n",
    "\n",
    "# d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 176, 176])\n",
    "# xd31 = relu(d31(xu33))\n",
    "\n",
    "# d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1).to(device) # torch.Size([10, 128, 174, 174])\n",
    "# xd32 = relu(d32(xd31))\n",
    "\n",
    "# upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2).to(device)  # torch.Size([10, 64, 364, 364])\n",
    "# xu4 = upconv4(xd32)\n",
    "\n",
    "# xu44 = torch.cat([xu4, xe12], dim=1) # torch.Size([10, 128, 364, 364])\n",
    "\n",
    "\n",
    "# d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1).to(device) \n",
    "# xd41 = relu(d41(xu44))\n",
    "\n",
    "# d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1).to(device) \n",
    "# xd42 = relu(d42(xd41))\n",
    "\n",
    "# outconv = nn.Conv2d(64, len(labels['AFG']), kernel_size=7, padding=5).to(device) \n",
    "# out = outconv(xd42)\n",
    "# print(out.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1) \n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1) \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1) \n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1) \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1) \n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1) \n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1) \n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1) \n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1) \n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1) \n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=7, padding=5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = relu(self.e21(xp1))\n",
    "        xe22 = relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = relu(self.e31(xp2))\n",
    "        xe32 = relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = relu(self.e41(xp3))\n",
    "        xe42 = relu(self.e42(xe41))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = relu(self.e51(xp4))\n",
    "        xe52 = relu(self.e52(xe51))\n",
    "        \n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52)\n",
    "        xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "        xd11 = relu(self.d11(xu11))\n",
    "        xd12 = relu(self.d12(xd11))\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "        xd21 = relu(self.d21(xu22))\n",
    "        xd22 = relu(self.d22(xd21))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xu33 = torch.cat([xu3, xe22], dim=1)\n",
    "        xd31 = relu(self.d31(xu33))\n",
    "        xd32 = relu(self.d32(xd31))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xu44 = torch.cat([xu4, xe12], dim=1)\n",
    "        xd41 = relu(self.d41(xu44))\n",
    "        xd42 = relu(self.d42(xd41))\n",
    "\n",
    "        # Output layer\n",
    "        out = F.avg_pool2d(self.outconv(xd42),(46,46),46) #, divisor_override=1)\n",
    "\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=5, padding=1) \n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=5, padding=1) \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=5, padding=1) \n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=5, padding=1) \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=5, padding=1) \n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=5, padding=1) \n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=5, padding=1) \n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=5, padding=1) \n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=5, padding=1) \n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=5, padding=1) \n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=10, stride=2)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=5, padding=1)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=5, padding=1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=19, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=5, padding=1)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=5, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=18, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=5, padding=1)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=5, padding=1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=18, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=7, padding=5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = relu(self.e21(xp1))\n",
    "        xe22 = relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = relu(self.e31(xp2))\n",
    "        xe32 = relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = relu(self.e41(xp3))\n",
    "        xe42 = relu(self.e42(xe41))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = relu(self.e51(xp4))\n",
    "        xe52 = relu(self.e52(xe51))\n",
    "        \n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52)\n",
    "        xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "        xd11 = relu(self.d11(xu11))\n",
    "        xd12 = relu(self.d12(xd11))\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "        xd21 = relu(self.d21(xu22))\n",
    "        xd22 = relu(self.d22(xd21))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xu33 = torch.cat([xu3, xe22], dim=1)\n",
    "        xd31 = relu(self.d31(xu33))\n",
    "        xd32 = relu(self.d32(xd31))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xu44 = torch.cat([xu4, xe12], dim=1)\n",
    "        xd41 = relu(self.d41(xu44))\n",
    "        xd42 = relu(self.d42(xd41))\n",
    "\n",
    "        # Output layer\n",
    "        out = F.avg_pool2d(self.outconv(xd42),(46,46),46) #, divisor_override=1)\n",
    "\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "source": [
    "model = UNet(n_class=len(labels['AFG'])).to(device)\n",
    "print(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "UNet(\n",
      "  (e11): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (e12): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (e21): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (e22): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (e31): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (e32): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (e41): Conv2d(256, 512, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (e42): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (e51): Conv2d(512, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (e52): Conv2d(1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (upconv1): ConvTranspose2d(1024, 512, kernel_size=(10, 10), stride=(2, 2))\n",
      "  (d11): Conv2d(1024, 512, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (d12): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (upconv2): ConvTranspose2d(512, 256, kernel_size=(19, 19), stride=(2, 2))\n",
      "  (d21): Conv2d(512, 256, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (d22): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (upconv3): ConvTranspose2d(256, 128, kernel_size=(18, 18), stride=(2, 2))\n",
      "  (d31): Conv2d(256, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (d32): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (upconv4): ConvTranspose2d(128, 64, kernel_size=(18, 18), stride=(2, 2))\n",
      "  (d41): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (d42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (outconv): Conv2d(64, 1, kernel_size=(7, 7), stride=(1, 1), padding=(5, 5))\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "source": [
    "summary(model, input_size=(3, 368, 368))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Conv2d: 1-1                            4,864\n",
      "├─Conv2d: 1-2                            102,464\n",
      "├─MaxPool2d: 1-3                         --\n",
      "├─Conv2d: 1-4                            204,928\n",
      "├─Conv2d: 1-5                            409,728\n",
      "├─MaxPool2d: 1-6                         --\n",
      "├─Conv2d: 1-7                            819,456\n",
      "├─Conv2d: 1-8                            1,638,656\n",
      "├─MaxPool2d: 1-9                         --\n",
      "├─Conv2d: 1-10                           3,277,312\n",
      "├─Conv2d: 1-11                           6,554,112\n",
      "├─MaxPool2d: 1-12                        --\n",
      "├─Conv2d: 1-13                           13,108,224\n",
      "├─Conv2d: 1-14                           26,215,424\n",
      "├─ConvTranspose2d: 1-15                  52,429,312\n",
      "├─Conv2d: 1-16                           13,107,712\n",
      "├─Conv2d: 1-17                           6,554,112\n",
      "├─ConvTranspose2d: 1-18                  47,317,248\n",
      "├─Conv2d: 1-19                           3,277,056\n",
      "├─Conv2d: 1-20                           1,638,656\n",
      "├─ConvTranspose2d: 1-21                  10,616,960\n",
      "├─Conv2d: 1-22                           819,328\n",
      "├─Conv2d: 1-23                           409,728\n",
      "├─ConvTranspose2d: 1-24                  2,654,272\n",
      "├─Conv2d: 1-25                           73,792\n",
      "├─Conv2d: 1-26                           36,928\n",
      "├─Conv2d: 1-27                           3,137\n",
      "=================================================================\n",
      "Total params: 191,273,409\n",
      "Trainable params: 191,273,409\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Conv2d: 1-1                            4,864\n",
       "├─Conv2d: 1-2                            102,464\n",
       "├─MaxPool2d: 1-3                         --\n",
       "├─Conv2d: 1-4                            204,928\n",
       "├─Conv2d: 1-5                            409,728\n",
       "├─MaxPool2d: 1-6                         --\n",
       "├─Conv2d: 1-7                            819,456\n",
       "├─Conv2d: 1-8                            1,638,656\n",
       "├─MaxPool2d: 1-9                         --\n",
       "├─Conv2d: 1-10                           3,277,312\n",
       "├─Conv2d: 1-11                           6,554,112\n",
       "├─MaxPool2d: 1-12                        --\n",
       "├─Conv2d: 1-13                           13,108,224\n",
       "├─Conv2d: 1-14                           26,215,424\n",
       "├─ConvTranspose2d: 1-15                  52,429,312\n",
       "├─Conv2d: 1-16                           13,107,712\n",
       "├─Conv2d: 1-17                           6,554,112\n",
       "├─ConvTranspose2d: 1-18                  47,317,248\n",
       "├─Conv2d: 1-19                           3,277,056\n",
       "├─Conv2d: 1-20                           1,638,656\n",
       "├─ConvTranspose2d: 1-21                  10,616,960\n",
       "├─Conv2d: 1-22                           819,328\n",
       "├─Conv2d: 1-23                           409,728\n",
       "├─ConvTranspose2d: 1-24                  2,654,272\n",
       "├─Conv2d: 1-25                           73,792\n",
       "├─Conv2d: 1-26                           36,928\n",
       "├─Conv2d: 1-27                           3,137\n",
       "=================================================================\n",
       "Total params: 191,273,409\n",
       "Trainable params: 191,273,409\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 145
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "source": [
    "loss_func = nn.L1Loss()\n",
    "iterator = iter(train_dl)\n",
    "\n",
    "for batch_idx in range(len(train_dl)):\n",
    "    data_batch = next(iterator)\n",
    "    xb = data_batch['obsvariable'].type(torch.float).to(device)\n",
    "    print(xb.shape)\n",
    "    yb = data_batch['groundtruth'].type(torch.float).to(device)\n",
    "    out = model(xb)\n",
    "    loss = loss_func(out, yb)\n",
    "    print(loss)\n",
    "    print(loss.item())\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 3, 368, 368])\n",
      "tensor(23.8162, device='mps:0', grad_fn=<MeanBackward0>)\n",
      "23.816186904907227\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "source": [
    "loss.backward()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/joshuadimasaka/miniconda3/envs/opensendaibench/lib/python3.11/site-packages/torch/autograd/__init__.py:251: UserWarning: The operator 'aten::sgn.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1702400235349/work/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "source": [
    "torch.device(\"mps\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "metadata": {},
     "execution_count": 148
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "source": [
    "opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "opt.step()\n",
    "opt.zero_grad()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "    def forward(self,yhat,y):\n",
    "        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n",
    "        return loss\n",
    "rmse = RMSELoss()\n",
    "def metrics_batch(target, output):\n",
    "    return rmse(output,target)\n",
    "def loss_batch(loss_func, xb, yb,yb_h, opt=None):\n",
    "    loss = loss_func(yb_h, yb)\n",
    "    metric_b = metrics_batch(yb,yb_h)\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    return loss.item(), metric_b\n",
    "def loss_epoch(model,loss_func,dataset_dl,opt=None):\n",
    "    loss=0.0\n",
    "    metric=0.0\n",
    "    iterator = iter(train_dl)\n",
    "    len_data = len(train_dl.dataset)\n",
    "    for batch_idx in range(len(train_dl)):\n",
    "        data_batch = next(iterator)\n",
    "        xb = data_batch['obsvariable'].type(torch.float).to(device)\n",
    "        yb = data_batch['groundtruth'].type(torch.float).to(device)\n",
    "        yb_h = model(xb)\n",
    "        loss_b,metric_b=loss_batch(loss_func, xb, yb,yb_h, opt)\n",
    "        loss+=loss_b\n",
    "        if metric_b is not None:\n",
    "            metric+=metric_b\n",
    "    loss/=len_data\n",
    "    metric/=len_data\n",
    "    return loss, metric\n",
    "def train_val(epochs, model, loss_func, opt, train_dl, val_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_metric=loss_epoch(model,loss_func,train_dl,opt)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_metric=loss_epoch(model,loss_func,val_dl)\n",
    "        accuracy=val_metric #100*val_metric\n",
    "        print(\"epoch: %d, train loss: %.6f, val loss: %.6f, rmse: %.2f\" %(epoch, train_loss,val_loss,accuracy))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "source": [
    "num_epochs=25\n",
    "train_val(num_epochs, model, loss_func, opt, train_dl, val_dl=train_dl)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 0, train loss: 0.584867, val loss: 0.255412, rmse: 0.48\n",
      "epoch: 1, train loss: 0.270989, val loss: 0.215523, rmse: 0.46\n",
      "epoch: 2, train loss: 0.235404, val loss: 0.366270, rmse: 0.54\n",
      "epoch: 3, train loss: 0.282199, val loss: 0.268134, rmse: 0.49\n",
      "epoch: 4, train loss: 0.327164, val loss: 0.227849, rmse: 0.41\n",
      "epoch: 5, train loss: 0.276471, val loss: 0.269979, rmse: 0.42\n",
      "epoch: 6, train loss: 0.232350, val loss: 0.222670, rmse: 0.42\n",
      "epoch: 7, train loss: 0.221526, val loss: 0.207861, rmse: 0.43\n",
      "epoch: 8, train loss: 0.212025, val loss: 0.213441, rmse: 0.45\n",
      "epoch: 9, train loss: 0.217688, val loss: 0.244440, rmse: 0.48\n",
      "epoch: 10, train loss: 0.225214, val loss: 0.212701, rmse: 0.42\n",
      "epoch: 11, train loss: 0.220745, val loss: 0.234991, rmse: 0.41\n",
      "epoch: 12, train loss: 0.217121, val loss: 0.225872, rmse: 0.46\n",
      "epoch: 13, train loss: 0.214155, val loss: 0.205140, rmse: 0.44\n",
      "epoch: 14, train loss: 0.212865, val loss: 0.215830, rmse: 0.42\n",
      "epoch: 15, train loss: 0.216309, val loss: 0.208230, rmse: 0.43\n",
      "epoch: 16, train loss: 0.208944, val loss: 0.205182, rmse: 0.43\n",
      "epoch: 17, train loss: 0.205198, val loss: 0.204579, rmse: 0.44\n",
      "epoch: 18, train loss: 0.205377, val loss: 0.209219, rmse: 0.42\n",
      "epoch: 19, train loss: 0.207157, val loss: 0.206604, rmse: 0.42\n",
      "epoch: 20, train loss: 0.208263, val loss: 0.204208, rmse: 0.43\n",
      "epoch: 21, train loss: 0.208416, val loss: 0.212720, rmse: 0.41\n",
      "epoch: 22, train loss: 0.208210, val loss: 0.205132, rmse: 0.42\n",
      "epoch: 23, train loss: 0.205419, val loss: 0.203696, rmse: 0.43\n",
      "epoch: 24, train loss: 0.204592, val loss: 0.203406, rmse: 0.43\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "source": [
    "path2weights=str(\"./models/weights_\"\n",
    "                 +datetime.now().strftime(\"%Y_%m_%d-%I_%M_%S_%p\")\n",
    "                 +\"_epoch_\"+str(num_epochs)+\".pt\")\n",
    "torch.save(model.state_dict(), path2weights)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "source": [
    "_model = UNet(n_class=len(labels['AFG']))\n",
    "weights=torch.load(path2weights)\n",
    "_model.load_state_dict(weights)\n",
    "_model.eval()\n",
    "_model.to(device)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (e11): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "  (e12): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (e21): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "  (e22): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (e31): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "  (e32): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (e41): Conv2d(256, 512, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "  (e42): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (e51): Conv2d(512, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "  (e52): Conv2d(1024, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "  (upconv1): ConvTranspose2d(1024, 512, kernel_size=(10, 10), stride=(2, 2))\n",
       "  (d11): Conv2d(1024, 512, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "  (d12): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "  (upconv2): ConvTranspose2d(512, 256, kernel_size=(19, 19), stride=(2, 2))\n",
       "  (d21): Conv2d(512, 256, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "  (d22): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "  (upconv3): ConvTranspose2d(256, 128, kernel_size=(18, 18), stride=(2, 2))\n",
       "  (d31): Conv2d(256, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "  (d32): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "  (upconv4): ConvTranspose2d(128, 64, kernel_size=(18, 18), stride=(2, 2))\n",
       "  (d41): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (d42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (outconv): Conv2d(64, 1, kernel_size=(7, 7), stride=(1, 1), padding=(5, 5))\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 153
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "source": [
    "n = 21\n",
    "x = train_ds[n]['obsvariable'].unsqueeze(0).type(torch.float).to(device)\n",
    "print(x.shape)\n",
    "y = train_ds[n]['groundtruth'].to(device)\n",
    "output=_model(x)\n",
    "print(output.shape)\n",
    "# fig, ((ax1,ax2,ax3,ax4,ax5),\n",
    "#       (ax6,ax7,ax8,ax9,ax10)) = plt.subplots(nrows=2, ncols=5)\n",
    "# ax1.imshow(output[0,0,:,:].cpu().detach().numpy())\n",
    "# ax2.imshow(output[0,1,:,:].cpu().detach().numpy())\n",
    "# ax3.imshow(output[0,2,:,:].cpu().detach().numpy())\n",
    "# ax4.imshow(output[0,3,:,:].cpu().detach().numpy())\n",
    "# ax5.imshow(output[0,4,:,:].cpu().detach().numpy())\n",
    "# ax6.imshow(y[0,:,:].cpu().detach().numpy())\n",
    "# ax7.imshow(y[1,:,:].cpu().detach().numpy())\n",
    "# ax8.imshow(y[2,:,:].cpu().detach().numpy())\n",
    "# ax9.imshow(y[3,:,:].cpu().detach().numpy())\n",
    "# ax10.imshow(y[4,:,:].cpu().detach().numpy())\n",
    "fig, (ax1, ax6) = plt.subplots(nrows=1, ncols=2)\n",
    "ax1.imshow(output[0,0,:,:].cpu().detach().numpy())\n",
    "ax6.imshow(y[0,:,:].cpu().detach().numpy())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 3, 368, 368])\n",
      "torch.Size([1, 1, 8, 8])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x4dc3080d0>"
      ]
     },
     "metadata": {},
     "execution_count": 154
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "source": [
    "n = 1\n",
    "x = train_ds[n]['obsvariable'].unsqueeze(0).type(torch.float).to(device)\n",
    "print(x.shape)\n",
    "y = train_ds[n]['groundtruth'].to(device)\n",
    "output=_model(x)\n",
    "print(output.shape)\n",
    "# fig, ((ax1,ax2,ax3,ax4,ax5),\n",
    "#       (ax6,ax7,ax8,ax9,ax10)) = plt.subplots(nrows=2, ncols=5)\n",
    "# ax1.imshow(output[0,0,:,:].cpu().detach().numpy())\n",
    "# ax2.imshow(output[0,1,:,:].cpu().detach().numpy())\n",
    "# ax3.imshow(output[0,2,:,:].cpu().detach().numpy())\n",
    "# ax4.imshow(output[0,3,:,:].cpu().detach().numpy())\n",
    "# ax5.imshow(output[0,4,:,:].cpu().detach().numpy())\n",
    "# ax6.imshow(y[0,:,:].cpu().detach().numpy())\n",
    "# ax7.imshow(y[1,:,:].cpu().detach().numpy())\n",
    "# ax8.imshow(y[2,:,:].cpu().detach().numpy())\n",
    "# ax9.imshow(y[3,:,:].cpu().detach().numpy())\n",
    "# ax10.imshow(y[4,:,:].cpu().detach().numpy())\n",
    "fig, (ax1, ax6) = plt.subplots(nrows=1, ncols=2)\n",
    "ax1.imshow(output[0,0,:,:].cpu().detach().numpy())\n",
    "ax6.imshow(y[0,:,:].cpu().detach().numpy())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 3, 368, 368])\n",
      "torch.Size([1, 1, 8, 8])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x4dc3611d0>"
      ]
     },
     "metadata": {},
     "execution_count": 155
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "source": [
    "n = 2\n",
    "x = train_ds[n]['obsvariable'].unsqueeze(0).type(torch.float).to(device)\n",
    "print(x.shape)\n",
    "y = train_ds[n]['groundtruth'].to(device)\n",
    "output=_model(x)\n",
    "print(output.shape)\n",
    "# fig, ((ax1,ax2,ax3,ax4,ax5),\n",
    "#       (ax6,ax7,ax8,ax9,ax10)) = plt.subplots(nrows=2, ncols=5)\n",
    "# ax1.imshow(output[0,0,:,:].cpu().detach().numpy())\n",
    "# ax2.imshow(output[0,1,:,:].cpu().detach().numpy())\n",
    "# ax3.imshow(output[0,2,:,:].cpu().detach().numpy())\n",
    "# ax4.imshow(output[0,3,:,:].cpu().detach().numpy())\n",
    "# ax5.imshow(output[0,4,:,:].cpu().detach().numpy())\n",
    "# ax6.imshow(y[0,:,:].cpu().detach().numpy())\n",
    "# ax7.imshow(y[1,:,:].cpu().detach().numpy())\n",
    "# ax8.imshow(y[2,:,:].cpu().detach().numpy())\n",
    "# ax9.imshow(y[3,:,:].cpu().detach().numpy())\n",
    "# ax10.imshow(y[4,:,:].cpu().detach().numpy())\n",
    "fig, (ax1, ax6) = plt.subplots(nrows=1, ncols=2)\n",
    "ax1.imshow(output[0,0,:,:].cpu().detach().numpy())\n",
    "ax6.imshow(y[0,:,:].cpu().detach().numpy())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 3, 368, 368])\n",
      "torch.Size([1, 1, 8, 8])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x4dc360590>"
      ]
     },
     "metadata": {},
     "execution_count": 156
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "source": [
    "n = 3\n",
    "x = train_ds[n]['obsvariable'].unsqueeze(0).type(torch.float).to(device)\n",
    "print(x.shape)\n",
    "y = train_ds[n]['groundtruth'].to(device)\n",
    "output=_model(x)\n",
    "print(output.shape)\n",
    "# fig, ((ax1,ax2,ax3,ax4,ax5),\n",
    "#       (ax6,ax7,ax8,ax9,ax10)) = plt.subplots(nrows=2, ncols=5)\n",
    "# ax1.imshow(output[0,0,:,:].cpu().detach().numpy())\n",
    "# ax2.imshow(output[0,1,:,:].cpu().detach().numpy())\n",
    "# ax3.imshow(output[0,2,:,:].cpu().detach().numpy())\n",
    "# ax4.imshow(output[0,3,:,:].cpu().detach().numpy())\n",
    "# ax5.imshow(output[0,4,:,:].cpu().detach().numpy())\n",
    "# ax6.imshow(y[0,:,:].cpu().detach().numpy())\n",
    "# ax7.imshow(y[1,:,:].cpu().detach().numpy())\n",
    "# ax8.imshow(y[2,:,:].cpu().detach().numpy())\n",
    "# ax9.imshow(y[3,:,:].cpu().detach().numpy())\n",
    "# ax10.imshow(y[4,:,:].cpu().detach().numpy())\n",
    "fig, (ax1, ax6) = plt.subplots(nrows=1, ncols=2)\n",
    "ax1.imshow(output[0,0,:,:].cpu().detach().numpy())\n",
    "ax6.imshow(y[0,:,:].cpu().detach().numpy())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 3, 368, 368])\n",
      "torch.Size([1, 1, 8, 8])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x5695706d0>"
      ]
     },
     "metadata": {},
     "execution_count": 157
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "source": [
    "n = 4\n",
    "x = train_ds[n]['obsvariable'].unsqueeze(0).type(torch.float).to(device)\n",
    "print(x.shape)\n",
    "y = train_ds[n]['groundtruth'].to(device)\n",
    "output=_model(x)\n",
    "print(output.shape)\n",
    "# fig, ((ax1,ax2,ax3,ax4,ax5),\n",
    "#       (ax6,ax7,ax8,ax9,ax10)) = plt.subplots(nrows=2, ncols=5)\n",
    "# ax1.imshow(output[0,0,:,:].cpu().detach().numpy())\n",
    "# ax2.imshow(output[0,1,:,:].cpu().detach().numpy())\n",
    "# ax3.imshow(output[0,2,:,:].cpu().detach().numpy())\n",
    "# ax4.imshow(output[0,3,:,:].cpu().detach().numpy())\n",
    "# ax5.imshow(output[0,4,:,:].cpu().detach().numpy())\n",
    "# ax6.imshow(y[0,:,:].cpu().detach().numpy())\n",
    "# ax7.imshow(y[1,:,:].cpu().detach().numpy())\n",
    "# ax8.imshow(y[2,:,:].cpu().detach().numpy())\n",
    "# ax9.imshow(y[3,:,:].cpu().detach().numpy())\n",
    "# ax10.imshow(y[4,:,:].cpu().detach().numpy())\n",
    "fig, (ax1, ax6) = plt.subplots(nrows=1, ncols=2)\n",
    "ax1.imshow(output[0,0,:,:].cpu().detach().numpy())\n",
    "ax6.imshow(y[0,:,:].cpu().detach().numpy())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 3, 368, 368])\n",
      "torch.Size([1, 1, 8, 8])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x569724110>"
      ]
     },
     "metadata": {},
     "execution_count": 158
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "source": [
    "n = 5\n",
    "x = train_ds[n]['obsvariable'].unsqueeze(0).type(torch.float).to(device)\n",
    "print(x.shape)\n",
    "y = train_ds[n]['groundtruth'].to(device)\n",
    "output=_model(x)\n",
    "print(output.shape)\n",
    "# fig, ((ax1,ax2,ax3,ax4,ax5),\n",
    "#       (ax6,ax7,ax8,ax9,ax10)) = plt.subplots(nrows=2, ncols=5)\n",
    "# ax1.imshow(output[0,0,:,:].cpu().detach().numpy())\n",
    "# ax2.imshow(output[0,1,:,:].cpu().detach().numpy())\n",
    "# ax3.imshow(output[0,2,:,:].cpu().detach().numpy())\n",
    "# ax4.imshow(output[0,3,:,:].cpu().detach().numpy())\n",
    "# ax5.imshow(output[0,4,:,:].cpu().detach().numpy())\n",
    "# ax6.imshow(y[0,:,:].cpu().detach().numpy())\n",
    "# ax7.imshow(y[1,:,:].cpu().detach().numpy())\n",
    "# ax8.imshow(y[2,:,:].cpu().detach().numpy())\n",
    "# ax9.imshow(y[3,:,:].cpu().detach().numpy())\n",
    "# ax10.imshow(y[4,:,:].cpu().detach().numpy())\n",
    "fig, (ax1, ax6) = plt.subplots(nrows=1, ncols=2)\n",
    "ax1.imshow(output[0,0,:,:].cpu().detach().numpy())\n",
    "ax6.imshow(y[0,:,:].cpu().detach().numpy())\n",
    "# %%"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 3, 368, 368])\n",
      "torch.Size([1, 1, 8, 8])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x569746150>"
      ]
     },
     "metadata": {},
     "execution_count": 159
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "source": [
    "n = 6\n",
    "x = train_ds[n]['obsvariable'].unsqueeze(0).type(torch.float).to(device)\n",
    "print(x.shape)\n",
    "y = train_ds[n]['groundtruth'].to(device)\n",
    "output=_model(x)\n",
    "print(output.shape)\n",
    "# fig, ((ax1,ax2,ax3,ax4,ax5),\n",
    "#       (ax6,ax7,ax8,ax9,ax10)) = plt.subplots(nrows=2, ncols=5)\n",
    "# ax1.imshow(output[0,0,:,:].cpu().detach().numpy())\n",
    "# ax2.imshow(output[0,1,:,:].cpu().detach().numpy())\n",
    "# ax3.imshow(output[0,2,:,:].cpu().detach().numpy())\n",
    "# ax4.imshow(output[0,3,:,:].cpu().detach().numpy())\n",
    "# ax5.imshow(output[0,4,:,:].cpu().detach().numpy())\n",
    "# ax6.imshow(y[0,:,:].cpu().detach().numpy())\n",
    "# ax7.imshow(y[1,:,:].cpu().detach().numpy())\n",
    "# ax8.imshow(y[2,:,:].cpu().detach().numpy())\n",
    "# ax9.imshow(y[3,:,:].cpu().detach().numpy())\n",
    "# ax10.imshow(y[4,:,:].cpu().detach().numpy())\n",
    "fig, (ax1, ax6) = plt.subplots(nrows=1, ncols=2)\n",
    "ax1.imshow(output[0,0,:,:].cpu().detach().numpy())\n",
    "ax6.imshow(y[0,:,:].cpu().detach().numpy())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 3, 368, 368])\n",
      "torch.Size([1, 1, 8, 8])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x56a8686d0>"
      ]
     },
     "metadata": {},
     "execution_count": 160
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "source": [
    "n = 7\n",
    "x = train_ds[n]['obsvariable'].unsqueeze(0).type(torch.float).to(device)\n",
    "print(x.shape)\n",
    "y = train_ds[n]['groundtruth'].to(device)\n",
    "output=_model(x)\n",
    "print(output.shape)\n",
    "# fig, ((ax1,ax2,ax3,ax4,ax5),\n",
    "#       (ax6,ax7,ax8,ax9,ax10)) = plt.subplots(nrows=2, ncols=5)\n",
    "# ax1.imshow(output[0,0,:,:].cpu().detach().numpy())\n",
    "# ax2.imshow(output[0,1,:,:].cpu().detach().numpy())\n",
    "# ax3.imshow(output[0,2,:,:].cpu().detach().numpy())\n",
    "# ax4.imshow(output[0,3,:,:].cpu().detach().numpy())\n",
    "# ax5.imshow(output[0,4,:,:].cpu().detach().numpy())\n",
    "# ax6.imshow(y[0,:,:].cpu().detach().numpy())\n",
    "# ax7.imshow(y[1,:,:].cpu().detach().numpy())\n",
    "# ax8.imshow(y[2,:,:].cpu().detach().numpy())\n",
    "# ax9.imshow(y[3,:,:].cpu().detach().numpy())\n",
    "# ax10.imshow(y[4,:,:].cpu().detach().numpy())\n",
    "fig, (ax1, ax6) = plt.subplots(nrows=1, ncols=2)\n",
    "ax1.imshow(output[0,0,:,:].cpu().detach().numpy())\n",
    "ax6.imshow(y[0,:,:].cpu().detach().numpy())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 3, 368, 368])\n",
      "torch.Size([1, 1, 8, 8])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x56a9591d0>"
      ]
     },
     "metadata": {},
     "execution_count": 161
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "source": [
    "n = 8\n",
    "x = train_ds[n]['obsvariable'].unsqueeze(0).type(torch.float).to(device)\n",
    "print(x.shape)\n",
    "y = train_ds[n]['groundtruth'].to(device)\n",
    "output=_model(x)\n",
    "print(output.shape)\n",
    "# fig, ((ax1,ax2,ax3,ax4,ax5),\n",
    "#       (ax6,ax7,ax8,ax9,ax10)) = plt.subplots(nrows=2, ncols=5)\n",
    "# ax1.imshow(output[0,0,:,:].cpu().detach().numpy())\n",
    "# ax2.imshow(output[0,1,:,:].cpu().detach().numpy())\n",
    "# ax3.imshow(output[0,2,:,:].cpu().detach().numpy())\n",
    "# ax4.imshow(output[0,3,:,:].cpu().detach().numpy())\n",
    "# ax5.imshow(output[0,4,:,:].cpu().detach().numpy())\n",
    "# ax6.imshow(y[0,:,:].cpu().detach().numpy())\n",
    "# ax7.imshow(y[1,:,:].cpu().detach().numpy())\n",
    "# ax8.imshow(y[2,:,:].cpu().detach().numpy())\n",
    "# ax9.imshow(y[3,:,:].cpu().detach().numpy())\n",
    "# ax10.imshow(y[4,:,:].cpu().detach().numpy())\n",
    "fig, (ax1, ax6) = plt.subplots(nrows=1, ncols=2)\n",
    "ax1.imshow(output[0,0,:,:].cpu().detach().numpy())\n",
    "ax6.imshow(y[0,:,:].cpu().detach().numpy())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 3, 368, 368])\n",
      "torch.Size([1, 1, 8, 8])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x56a9fe750>"
      ]
     },
     "metadata": {},
     "execution_count": 162
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "source": [
    "n = 9\n",
    "x = train_ds[n]['obsvariable'].unsqueeze(0).type(torch.float).to(device)\n",
    "print(x.shape)\n",
    "y = train_ds[n]['groundtruth'].to(device)\n",
    "output=_model(x)\n",
    "print(output.shape)\n",
    "# fig, ((ax1,ax2,ax3,ax4,ax5),\n",
    "#       (ax6,ax7,ax8,ax9,ax10)) = plt.subplots(nrows=2, ncols=5)\n",
    "# ax1.imshow(output[0,0,:,:].cpu().detach().numpy())\n",
    "# ax2.imshow(output[0,1,:,:].cpu().detach().numpy())\n",
    "# ax3.imshow(output[0,2,:,:].cpu().detach().numpy())\n",
    "# ax4.imshow(output[0,3,:,:].cpu().detach().numpy())\n",
    "# ax5.imshow(output[0,4,:,:].cpu().detach().numpy())\n",
    "# ax6.imshow(y[0,:,:].cpu().detach().numpy())\n",
    "# ax7.imshow(y[1,:,:].cpu().detach().numpy())\n",
    "# ax8.imshow(y[2,:,:].cpu().detach().numpy())\n",
    "# ax9.imshow(y[3,:,:].cpu().detach().numpy())\n",
    "# ax10.imshow(y[4,:,:].cpu().detach().numpy())\n",
    "fig, (ax1, ax6) = plt.subplots(nrows=1, ncols=2)\n",
    "ax1.imshow(output[0,0,:,:].cpu().detach().numpy())\n",
    "ax6.imshow(y[0,:,:].cpu().detach().numpy())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 3, 368, 368])\n",
      "torch.Size([1, 1, 8, 8])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x56aad0590>"
      ]
     },
     "metadata": {},
     "execution_count": 163
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "source": [
    "n = 10\n",
    "x = train_ds[n]['obsvariable'].unsqueeze(0).type(torch.float).to(device)\n",
    "print(x.shape)\n",
    "y = train_ds[n]['groundtruth'].to(device)\n",
    "output=_model(x)\n",
    "print(output.shape)\n",
    "# fig, ((ax1,ax2,ax3,ax4,ax5),\n",
    "#       (ax6,ax7,ax8,ax9,ax10)) = plt.subplots(nrows=2, ncols=5)\n",
    "# ax1.imshow(output[0,0,:,:].cpu().detach().numpy())\n",
    "# ax2.imshow(output[0,1,:,:].cpu().detach().numpy())\n",
    "# ax3.imshow(output[0,2,:,:].cpu().detach().numpy())\n",
    "# ax4.imshow(output[0,3,:,:].cpu().detach().numpy())\n",
    "# ax5.imshow(output[0,4,:,:].cpu().detach().numpy())\n",
    "# ax6.imshow(y[0,:,:].cpu().detach().numpy())\n",
    "# ax7.imshow(y[1,:,:].cpu().detach().numpy())\n",
    "# ax8.imshow(y[2,:,:].cpu().detach().numpy())\n",
    "# ax9.imshow(y[3,:,:].cpu().detach().numpy())\n",
    "# ax10.imshow(y[4,:,:].cpu().detach().numpy())\n",
    "fig, (ax1, ax6) = plt.subplots(nrows=1, ncols=2)\n",
    "ax1.imshow(output[0,0,:,:].cpu().detach().numpy())\n",
    "ax6.imshow(y[0,:,:].cpu().detach().numpy())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 3, 368, 368])\n",
      "torch.Size([1, 1, 8, 8])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x56abd4b90>"
      ]
     },
     "metadata": {},
     "execution_count": 164
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAENCAYAAABTviwWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAYSElEQVR4nO3dfXBU9b3H8c9uHjYkJIuAQRi2yChXwEhFgpUHnzWdDDraGZnWQaWPUzQgNNPRpv7Rjr2y+kcd27HmCsPQYVBhegeQzi1guJaI1xuFICMVL2BBs/IgxeouLMOGZM/94zbpjYDkbH7725xz3q+Z88euJ/P9LpCPn+xu9oQcx3EEAABgQLjQCwAAAP+gWAAAAGMoFgAAwBiKBQAAMIZiAQAAjKFYAAAAYygWAADAGIoFAAAwptj2wGw2qyNHjqiyslKhUMj2eCDwHMfRyZMnNWbMGIXD3vjZgtwACq+/2WG9WBw5ckSxWMz2WABfkkgkNHbs2EKv0S/kBjB4XCw7rBeLyspKSdLY3z6m8JCIlZnho2VW5vTorshanSdJTsjuJ7OXHbX7T6coY3Wchh3ssjtQ0tB9n1uZ09WdUevBF3q/F72gZ9ebyu9TcajEyszw0KFW5vTInjpldV4h2P4z7fr0uNV5hRCuKLc2q8s5qzdO//tFs8N6seh5GjM8JKJwuZ3/4YfL7BYLZ4j/i0VRxHKxsDpNKi6xXyyKi+wU7R5eekmhZ9fiUImKQ6VWZobDdub0yFp6XIVk+89UlkpoIYUL8O/mYtnhjRdYAQCAJ1AsAACAMRQLAABgDMUCAAAYQ7EAAADGUCwAAIAxFAsAAGBMTsXihRde0Pjx41VWVqZp06Zp+/btpvcC4DPkBhAMrovF2rVrtWTJEj3xxBN69913deONN6q+vl4dHR352A+AD5AbQHC4LhbPPvusfvCDH+iHP/yhJk2apOeee06xWEzNzc352A+AD5AbQHC4KhadnZ1qb29XXV1dn/vr6ur01ltvnfdrMpmMUqlUnwNAcJAbQLC4KhYnTpxQd3e3Ro0a1ef+UaNG6dixY+f9mng8rmg02ntwhUIgWMgNIFhyevPmly9A4jjOBS9K0tTUpGQy2XskEolcRgLwOHIDCAZXl6gcOXKkioqKzvkp4/jx4+f8NNIjEokoErF71UYAgwe5AQSLq2csSktLNW3aNLW0tPS5v6WlRTNnzjS6GAB/IDeAYHH1jIUkNTY26sEHH1Rtba1mzJihZcuWqaOjQwsWLMjHfgB8gNwAgsN1sfj2t7+tzz77TE8++aSOHj2qmpoa/elPf9K4cePysR8AHyA3gOBwXSwk6ZFHHtEjjzxiehcAPkZuAMHAtUIAAIAxFAsAAGAMxQIAABhDsQAAAMZQLAAAgDEUCwAAYExOv25qQvl7Q1QUKbMy68yljpU5Pco7iqzOkySd/5ILeVP2N7t/ptFDnVbnFZ3psjpPkrJVQ+zM6bb8jwX9Eq4can1m9uQpX88Lgmw6bW+Wc7Zf5/GMBQAAMIZiAQAAjKFYAAAAYygWAADAGIoFAAAwhmIBAACMoVgAAABjKBYAAMAYigUAADCGYgEAAIxxXSzeeOMN3X333RozZoxCoZA2bNiQh7UA+Am5AQSH62KRTqf19a9/Xc8//3w+9gHgQ+QGEByuL0JWX1+v+vr6fOwCwKfIDSA48n5100wmo0wm03s7lUrleyQAjyM3AO/K+5s34/G4otFo7xGLxfI9EoDHkRuAd+W9WDQ1NSmZTPYeiUQi3yMBeBy5AXhX3l8KiUQiikQi+R4DwEfIDcC7+BwLAABgjOtnLE6dOqUPP/yw9/ahQ4e0e/duDR8+XF/72teMLgfAH8gNIDhcF4udO3fq1ltv7b3d2NgoSZo/f75+//vfG1sMgH+QG0BwuC4Wt9xyixzHyccuAHyK3ACCg/dYAAAAYygWAADAGIoFAAAwhmIBAACMoVgAAABjKBYAAMCYvH+k94VMuXevSipKrczataHGypweoW6r4yRJ6ZrMxU8yqHqn1XGKdPzd6jynzP7HSTtDSqzPxIX9x64tVued6E5bnSdJD936gPWZNoWPHrc6L5u2/3c4GPGMBQAAMIZiAQAAjKFYAAAAYygWAADAGIoFAAAwhmIBAACMoVgAAABjKBYAAMAYigUAADCGYgEAAIxxVSzi8bimT5+uyspKVVdX695779W+ffvytRsAnyA7gOBwVSxaW1vV0NCgtrY2tbS0qKurS3V1dUrz+egAvgLZAQSHq4uQbd68uc/tlStXqrq6Wu3t7brpppuMLgbAP8gOIDgGdHXTZDIpSRo+fPgFz8lkMspk/nnlzVQqNZCRAHzgYtlBbgDelfObNx3HUWNjo2bPnq2amgtfljwejysajfYesVgs15EAfKA/2UFuAN6Vc7FYuHCh3nvvPb3yyitfeV5TU5OSyWTvkUgkch0JwAf6kx3kBuBdOb0UsmjRIm3cuFFvvPGGxo4d+5XnRiIRRSKRnJYD4C/9zQ5yA/AuV8XCcRwtWrRI69ev17Zt2zR+/Ph87QXAR8gOIDhcFYuGhga9/PLLevXVV1VZWaljx45JkqLRqIYMGZKXBQF4H9kBBIer91g0NzcrmUzqlltu0ejRo3uPtWvX5ms/AD5AdgDB4fqlEABwi+wAgoNrhQAAAGMoFgAAwBiKBQAAMIZiAQAAjKFYAAAAYygWAADAGIoFAAAwZkCXTR+Itz64UuEhZVZmjfyb3d+hT15pdZwk6foJh6zOS1wyweq8oswwq/OypfY7d+ToSStzwt2dVubkQzZ9WtnQWSuzvjnmWitzeryU+C+r8yTJqbCTwT1C6TNW52XTaavzwhUVVudJ9h9jf/CMBQAAMIZiAQAAjKFYAAAAYygWAADAGIoFAAAwhmIBAACMoVgAAABjKBYAAMAYigUAADDGVbFobm7WlClTVFVVpaqqKs2YMUObNm3K124AfILsAILDVbEYO3asnn76ae3cuVM7d+7UbbfdpnvuuUfvv/9+vvYD4ANkBxAcrq4Vcvfdd/e5/dRTT6m5uVltbW26+uqrjS4GwD/IDiA4cr4IWXd3t/7whz8onU5rxowZFzwvk8kok8n03k6lUrmOBOAD/ckOcgPwLtdv3tyzZ4+GDh2qSCSiBQsWaP369Zo8efIFz4/H44pGo71HLBYb0MIAvMlNdpAbgHe5LhZXXXWVdu/erba2Nj388MOaP3++9u7de8Hzm5qalEwme49EIjGghQF4k5vsIDcA73L9UkhpaamuvPJKSVJtba127Nih3/zmN3rxxRfPe34kElEkEhnYlgA8z012kBuAdw34cywcx+nzWigA9AfZAfiTq2csfv7zn6u+vl6xWEwnT57UmjVrtG3bNm3evDlf+wHwAbIDCA5XxeLTTz/Vgw8+qKNHjyoajWrKlCnavHmz7rzzznztB8AHyA4gOFwVixUrVuRrDwA+RnYAwcG1QgAAgDEUCwAAYAzFAgAAGEOxAAAAxlAsAACAMRQLAABgDMUCAAAYk/Nl0wesMywV2ek1Q/7ebWVOjy+Ki6zOk6Rd26+yOi861Oo4DSkKWZ1XdLrL6jxJCiVP2ZmT9e7HaIcryhUOlRZ6jbx4cGJdAaZ2WJ3mWJ1mXzadtj4zXFFhb5bTKfXjIfKMBQAAMIZiAQAAjKFYAAAAYygWAADAGIoFAAAwhmIBAACMoVgAAABjKBYAAMAYigUAADBmQMUiHo8rFAppyZIlhtYB4HfkBuBvOReLHTt2aNmyZZoyZYrJfQD4GLkB+F9OxeLUqVOaN2+eli9frksuucT0TgB8iNwAgiGnYtHQ0KA5c+bojjvuML0PAJ8iN4BgcH110zVr1mjXrl3asWNHv87PZDLKZP55NcVUKuV2JACPIzeA4HD1jEUikdDixYu1evVqlZWV9etr4vG4otFo7xGLxXJaFIA3kRtAsIQcx3H6e/KGDRv0rW99S0VFRb33dXd3KxQKKRwOK5PJ9Plv0vl/8ojFYhr77JMKD+lfyAxU7DUrY3odnVl08ZMMC2XtzovuszzvUObiJxkUznRbnSdJJR1/szKnK5vR1iMvKplMqqqqKu/zTObGbRX3qzhUmved4Q/ZdLrQK+RduKLC2qwup1Ovp1+5aHa4eink9ttv1549e/rc973vfU8TJ07U448/fk44SFIkElEkEnEzBoCPkBtAsLgqFpWVlaqpqelzX0VFhUaMGHHO/QAgkRtA0PDJmwAAwBjXvxXyZdu2bTOwBoAgITcA/+IZCwAAYAzFAgAAGEOxAAAAxlAsAACAMRQLAABgDMUCAAAYQ7EAAADGDPhzLHIVOhtWqNhSr3HsXkij6IzVcZKkkdd/anXe4REjrM5TyO7HO5ec7vcldIy55FiJnUFZyxeWMSibPq1s6KyVWTavwQDkyub1ULJO/773eMYCAAAYQ7EAAADGUCwAAIAxFAsAAGAMxQIAABhDsQAAAMZQLAAAgDEUCwAAYAzFAgAAGEOxAAAAxrgqFr/85S8VCoX6HJdddlm+dgPgE2QHEByurxVy9dVXa+vWrb23i4qKjC4EwJ/IDiAYXBeL4uJiftIA4BrZAQSD6/dYHDhwQGPGjNH48eP1ne98RwcPHvzK8zOZjFKpVJ8DQPC4yQ5yA/AuV8XiG9/4hlatWqUtW7Zo+fLlOnbsmGbOnKnPPvvsgl8Tj8cVjUZ7j1gsNuClAXiL2+wgNwDvCjmO4+T6xel0WldccYUee+wxNTY2nvecTCajTCbTezuVSikWiyn2zL8qPKQs19GujH09a2VOj+PT7L92PPL6T63OO5wYYXXeyP92/ardgJSczvnbImeXvH3UypyubEZbP3peyWRSVVVVVmZ+2cWy40K5cYvuUXGoxMqO4YoKK3OQP9l0utAr+EqXc1bb9OpFs2NAaV1RUaFrrrlGBw4cuOA5kUhEkUhkIGMA+MzFsoPcALxrQJ9jkclk9MEHH2j06NGm9gEQAGQH4F+uisVPf/pTtba26tChQ3r77bd13333KZVKaf78+fnaD4APkB1AcLh6KeSTTz7R/fffrxMnTujSSy/VDTfcoLa2No0bNy5f+wHwAbIDCA5XxWLNmjX52gOAj5EdQHBwrRAAAGAMxQIAABhDsQAAAMZQLAAAgDEUCwAAYAzFAgAAGGP3Agz/z4za/1FJRamVWe+Mtvu78pkv7FwD5f97c8o6q/My15y1Om/+5G9anffx8/9idZ4kdR362M4cx+7fnUnhinKFQ3ZyIwg6b5hY6BXyqvg/2wu9Qt7ZvKZN2OmU+nH5FZ6xAAAAxlAsAACAMRQLAABgDMUCAAAYQ7EAAADGUCwAAIAxFAsAAGAMxQIAABhDsQAAAMZQLAAAgDGui8Xhw4f1wAMPaMSIESovL9e1116r9nb/f2wqgIEhO4BgcHWtkM8//1yzZs3Srbfeqk2bNqm6ulp//etfNWzYsDytB8APyA4gOFwVi2eeeUaxWEwrV67sve/yyy83vRMAnyE7gOBw9VLIxo0bVVtbq7lz56q6ulpTp07V8uXLv/JrMpmMUqlUnwNAsLjNDnID8C5XxeLgwYNqbm7WhAkTtGXLFi1YsECPPvqoVq1adcGvicfjikajvUcsFhvw0gC8xW12kBuAd7kqFtlsVtddd52WLl2qqVOn6sc//rF+9KMfqbm5+YJf09TUpGQy2XskEokBLw3AW9xmB7kBeJerYjF69GhNnjy5z32TJk1SR0fHBb8mEomoqqqqzwEgWNxmB7kBeJerYjFr1izt27evz3379+/XuHHjjC4FwF/IDiA4XBWLn/zkJ2pra9PSpUv14Ycf6uWXX9ayZcvU0NCQr/0A+ADZAQSHq2Ixffp0rV+/Xq+88opqamr0q1/9Ss8995zmzZuXr/0A+ADZAQSHq8+xkKS77rpLd911Vz52AeBjZAcQDFwrBAAAGEOxAAAAxlAsAACAMRQLAABgDMUCAAAYQ7EAAADGUCwAAIAxrj/HwpQ96yarKFJmZVbV3x0rc3rcvLjN6jxJumt/vdV5N4/cb3Ve+/arrM67LJO1Ok+SPn10ppU53Zkz0r+9amUWgPzKptP2Zjln+3Uez1gAAABjKBYAAMAYigUAADCGYgEAAIyhWAAAAGMoFgAAwBiKBQAAMIZiAQAAjKFYAAAAY1wVi8svv1yhUOico6GhIV/7AfABsgMIDlcf6b1jxw51d3f33v7LX/6iO++8U3PnzjW+GAD/IDuA4HBVLC699NI+t59++mldccUVuvnmm40uBcBfyA4gOHK+CFlnZ6dWr16txsZGhUKhC56XyWSUyWR6b6dSqVxHAvCB/mQHuQF4V85v3tywYYO++OILffe73/3K8+LxuKLRaO8Ri8VyHQnAB/qTHeQG4F05F4sVK1aovr5eY8aM+crzmpqalEwme49EIpHrSAA+0J/sIDcA78rppZCPP/5YW7du1bp16y56biQSUSQSyWUMAJ/pb3aQG4B35fSMxcqVK1VdXa05c+aY3geAj5EdgP+5LhbZbFYrV67U/PnzVVyc83s/AQQM2QEEg+tisXXrVnV0dOj73/9+PvYB4FNkBxAMrn9sqKurk+M4+dgFgI+RHUAwcK0QAABgDMUCAAAYQ7EAAADGUCwAAIAxFAsAAGAMxQIAABhj/VNqen7drLvzjLWZ3Z12f8Utc+qs1XmSdDbdaXXembIuq/OyZ+z9e5GkrrNZq/MkqTtTZGfOP773vPSrnz27djn2v7f8rKvL7veVdfx7MapL//fnebHsCDmW0+WTTz7hSoXAIJBIJDR27NhCr9Ev5AYweFwsO6wXi2w2qyNHjqiyslKhUKjfX5dKpRSLxZRIJFRVVZXHDQuDx+d9XnmMjuPo5MmTGjNmjMJhb7waSm5cmN8fI49v8Ohvdlh/KSQcDg/op6SqqqpB/4c/EDw+7/PCY4xGo4VewRVy4+L8/hh5fINDf7LDGz+uAAAAT6BYAAAAYzxTLCKRiH7xi18oEokUepW84PF5XxAeo9cE4e/E74+Rx+c91t+8CQAA/Mszz1gAAIDBj2IBAACMoVgAAABjKBYAAMAYTxSLF154QePHj1dZWZmmTZum7du3F3olY+LxuKZPn67KykpVV1fr3nvv1b59+wq9Vt7E43GFQiEtWbKk0KsYc/jwYT3wwAMaMWKEysvLde2116q9vb3Qa0H+zQ5ywx/8mh2DvlisXbtWS5Ys0RNPPKF3331XN954o+rr69XR0VHo1YxobW1VQ0OD2tra1NLSoq6uLtXV1SmdThd6NeN27NihZcuWacqUKYVexZjPP/9cs2bNUklJiTZt2qS9e/fq17/+tYYNG1bo1QLPz9lBbnifr7PDGeSuv/56Z8GCBX3umzhxovOzn/2sQBvl1/Hjxx1JTmtra6FXMerkyZPOhAkTnJaWFufmm292Fi9eXOiVjHj88ced2bNnF3oNnEeQsoPc8B4/Z8egfsais7NT7e3tqqur63N/XV2d3nrrrQJtlV/JZFKSNHz48AJvYlZDQ4PmzJmjO+64o9CrGLVx40bV1tZq7ty5qq6u1tSpU7V8+fJCrxV4QcsOcsN7/Jwdg7pYnDhxQt3d3Ro1alSf+0eNGqVjx44VaKv8cRxHjY2Nmj17tmpqagq9jjFr1qzRrl27FI/HC72KcQcPHlRzc7MmTJigLVu2aMGCBXr00Ue1atWqQq8WaEHKDnLDm/ycHdavbpqLL18m2XEcV5dO9oqFCxfqvffe05tvvlnoVYxJJBJavHixXnvtNZWVlRV6HeOy2axqa2u1dOlSSdLUqVP1/vvvq7m5WQ899FCBt0MQsoPc8CY/Z8egfsZi5MiRKioqOucnjOPHj5/zk4jXLVq0SBs3btSf//znAV0eerBpb2/X8ePHNW3aNBUXF6u4uFitra367W9/q+LiYnV3dxd6xQEZPXq0Jk+e3Oe+SZMm+eINgl4WlOwgN7zLz9kxqItFaWmppk2bppaWlj73t7S0aObMmQXayizHcbRw4UKtW7dOr7/+usaPH1/olYy6/fbbtWfPHu3evbv3qK2t1bx587R7924VFRUVesUBmTVr1jm/5rd//36NGzeuQBtB8n92kBvezg3J59lRyHeO9seaNWuckpISZ8WKFc7evXudJUuWOBUVFc5HH31U6NWMePjhh51oNOps27bNOXr0aO9x+vTpQq+WN356d/c777zjFBcXO0899ZRz4MAB56WXXnLKy8ud1atXF3q1wPNzdpAb3ufn7Bj0xcJxHOd3v/udM27cOKe0tNS57rrrfPUrVZLOe6xcubLQq+WN3wLij3/8o1NTU+NEIhFn4sSJzrJlywq9Ev7Br9lBbviDX7ODy6YDAABjBvV7LAAAgLdQLAAAgDEUCwAAYAzFAgAAGEOxAAAAxlAsAACAMRQLAABgDMUCAAAYQ7EAAADGUCwAAIAxFAsAAGAMxQIAABjzv9Q/WPqTyROzAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 }
}